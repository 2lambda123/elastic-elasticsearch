/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License;
 * you may not use this file except in compliance with the Elastic License.
 */

import org.apache.tools.ant.taskdefs.condition.Os
import org.elasticsearch.gradle.info.BuildParams
import org.elasticsearch.gradle.test.RestIntegTestTask

import java.nio.file.Files
import java.nio.file.Paths

import static org.elasticsearch.gradle.PropertyNormalization.IGNORE_VALUE

apply plugin: 'elasticsearch.test.fixtures'
apply plugin: 'elasticsearch.standalone-rest-test'
apply plugin: 'elasticsearch.rest-test'
apply plugin: 'elasticsearch.rest-resources'

final Project hdfsFixtureProject = project(':test:fixtures:hdfs-fixture')
final Project krbFixtureProject = project(':test:fixtures:krb5kdc-fixture')
final Project hdfsRepoPluginProject = project(':plugins:repository-hdfs')

dependencies {
  testImplementation project(path: xpackModule('searchable-snapshots'), configuration: 'testArtifacts')
  testImplementation hdfsRepoPluginProject
}

restResources {
  restApi {
    includeCore 'indices', 'search', 'bulk', 'snapshot', 'nodes', '_common'
    includeXpack 'searchable_snapshots'
  }
}

testFixtures.useFixture(krbFixtureProject.path, 'hdfs-snapshot')

configurations {
  hdfsFixture
}

dependencies {
  hdfsFixture hdfsFixtureProject
  // Set the keytab files in the classpath so that we can access them from test code without the security manager freaking out.
  if (isEclipse == false) {
    testRuntimeOnly files(krbFixtureProject.ext.krb5Keytabs("hdfs-snapshot", "hdfs_hdfs.build.elastic.co.keytab").parent)
  }
}

normalization {
  runtimeClasspath {
    // ignore generated keytab files for the purposes of build avoidance
    ignore '*.keytab'
    // ignore fixture ports file which is on the classpath primarily to pacify the security manager
    ignore 'ports'
  }
}

String realm = "BUILD.ELASTIC.CO"
String krb5conf = krbFixtureProject.ext.krb5Conf("hdfs")

// Create HDFS File System Testing Fixtures for HA/Secure combinations
// TODO: Do we need HA fixtures? Unlikely?
//for (String fixtureName : ['hdfsFixture', 'haHdfsFixture', 'secureHdfsFixture', 'secureHaHdfsFixture']) {
for (String fixtureName : ['hdfsFixture', 'secureHdfsFixture']) {
  def tsk = project.tasks.register(fixtureName, org.elasticsearch.gradle.test.AntFixture) {
    dependsOn project.configurations.hdfsFixture, krbFixtureProject.tasks.postProcessFixture
    executable = "${BuildParams.runtimeJavaHome}/bin/java"
    env 'CLASSPATH', "${-> project.configurations.hdfsFixture.asPath}"
    maxWaitInSeconds 60
    onlyIf { BuildParams.inFipsJvm == false }
    waitCondition = { fixture, ant ->
      // the hdfs.MiniHDFS fixture writes the ports file when
      // it's ready, so we can just wait for the file to exist
      return fixture.portsFile.exists()
    }
    final List<String> miniHDFSArgs = []

    // If it's a secure fixture, then depend on Kerberos Fixture and principals + add the krb5conf to the JVM options
    if (fixtureName.equals('secureHdfsFixture') || fixtureName.equals('secureHaHdfsFixture')) {
      miniHDFSArgs.add("-Djava.security.krb5.conf=${krb5conf}")
    }
//    // If it's an HA fixture, set a nameservice to use in the JVM options
//    if (fixtureName.equals('haHdfsFixture') || fixtureName.equals('secureHaHdfsFixture')) {
//      miniHDFSArgs.add("-Dha-nameservice=ha-hdfs")
//    }

    // Common options
    miniHDFSArgs.add('hdfs.MiniHDFS')
    miniHDFSArgs.add(baseDir)

    // If it's a secure fixture, then set the principal name and keytab locations to use for auth.
    if (fixtureName.equals('secureHdfsFixture') || fixtureName.equals('secureHaHdfsFixture')) {
      miniHDFSArgs.add("hdfs/hdfs.build.elastic.co@${realm}")
      miniHDFSArgs.add(
        project(':test:fixtures:krb5kdc-fixture').ext.krb5Keytabs("hdfs", "hdfs_hdfs.build.elastic.co.keytab")
      )
    }

    args miniHDFSArgs.toArray()
  }

  // TODO: The task configuration block has side effects that require it currently to be always executed.
  // Otherwise tests start failing. Therefore we enforce the task creation for now.
  tsk.get()
}

Set disabledIntegTestTaskNames = []

testClusters.matching { it.name == "integTest" }.configureEach {
  testDistribution = 'DEFAULT'
  plugin(hdfsRepoPluginProject.path)
  setting 'xpack.license.self_generated.type', 'trial'
}

// FIXHERE Do we need HA testing for searchable snapshots? I imagine that it is not vitally important...
//for (String integTestTaskName : ['integTestHa', 'integTestSecure', 'integTestSecureHa']) {
for (String integTestTaskName : ['integTestSecure']) {
  task "${integTestTaskName}"(type: RestIntegTestTask) {
    description = "Runs rest tests against an elasticsearch cluster with HDFS."

    if (disabledIntegTestTaskNames.contains(integTestTaskName)) {
      enabled = false
    }

//    if (integTestTaskName.contains("Secure")) {
//      if (integTestTaskName.contains("Ha")) {
//        dependsOn secureHaHdfsFixture
//      } else {
        dependsOn secureHdfsFixture
//      }
//    }

    // TODO: If secure
    systemProperty 'test.hdfs.uri', 'hdfs://localhost:9998'
    nonInputProperties.systemProperty 'test.hdfs.path', '/user/elasticsearch/test/searchable_snapshots/secure'

    onlyIf { BuildParams.inFipsJvm == false }
//    if (integTestTaskName.contains("Ha")) {
//      java.nio.file.Path portsFile
//      File portsFileDir = file("${workingDir}/hdfsFixture")
//      if (integTestTaskName.contains("Secure")) {
//        portsFile = buildDir.toPath()
//          .resolve("fixtures")
//          .resolve("secureHaHdfsFixture")
//          .resolve("ports")
//      } else {
//        portsFile = buildDir.toPath()
//          .resolve("fixtures")
//          .resolve("haHdfsFixture")
//          .resolve("ports")
//      }
//      nonInputProperties.systemProperty "test.hdfs-fixture.ports", file("$portsFileDir/ports")
//      classpath += files(portsFileDir)
//      // Copy ports file to separate location which is placed on the test classpath
//      doFirst {
//        mkdir(portsFileDir)
//        copy {
//          from portsFile
//          into portsFileDir
//        }
//      }
//    }

//    if (integTestTaskName.contains("Secure")) {
      if (disabledIntegTestTaskNames.contains(integTestTaskName) == false) {
        nonInputProperties.systemProperty "test.krb5.principal.es", "elasticsearch@${realm}"
        nonInputProperties.systemProperty "test.krb5.principal.hdfs", "hdfs/hdfs.build.elastic.co@${realm}"
        nonInputProperties.systemProperty(
          "test.krb5.keytab.hdfs",
          project(':test:fixtures:krb5kdc-fixture').ext.krb5Keytabs("hdfs", "hdfs_hdfs.build.elastic.co.keytab")
        )
      }
//    }


  }

  testClusters."${integTestTaskName}" {
    testDistribution = 'DEFAULT'
    plugin(hdfsRepoPluginProject.path)
    setting 'xpack.license.self_generated.type', 'trial'
    if (integTestTaskName.contains("Secure")) {
      systemProperty "java.security.krb5.conf", krb5conf
      extraConfigFile(
        "repository-hdfs/krb5.keytab",
        file("${project(':test:fixtures:krb5kdc-fixture').ext.krb5Keytabs("hdfs", "elasticsearch.keytab")}"), IGNORE_VALUE
      )
    }
  }
}

// Determine HDFS Fixture compatibility for the current build environment.
boolean fixtureSupported = false
if (Os.isFamily(Os.FAMILY_WINDOWS)) {
  // hdfs fixture will not start without hadoop native libraries on windows
  String nativePath = System.getenv("HADOOP_HOME")
  if (nativePath != null) {
    java.nio.file.Path path = Paths.get(nativePath);
    if (Files.isDirectory(path) &&
      Files.exists(path.resolve("bin").resolve("winutils.exe")) &&
      Files.exists(path.resolve("bin").resolve("hadoop.dll")) &&
      Files.exists(path.resolve("bin").resolve("hdfs.dll"))) {
      fixtureSupported = true
    } else {
      throw new IllegalStateException("HADOOP_HOME: ${path} is invalid, does not contain hadoop native libraries in \$HADOOP_HOME/bin");
    }
  }
} else {
  fixtureSupported = true
}

boolean legalPath = rootProject.rootDir.toString().contains(" ") == false
if (legalPath == false) {
  fixtureSupported = false
}

// Disable integration test if Fips mode
integTest {
  onlyIf { BuildParams.inFipsJvm == false }
}

if (fixtureSupported) {
  // Check depends on the HA test. Already depends on the standard test.
//  project.check.dependsOn(integTestHa)

  // Both standard and HA tests depend on their respective HDFS fixtures
  integTest.dependsOn hdfsFixture
//  integTestHa.dependsOn haHdfsFixture

  // The normal test runner only runs the standard hdfs rest tests
  integTest {
    systemProperty 'test.hdfs.uri', 'hdfs://localhost:9999'
    nonInputProperties.systemProperty 'test.hdfs.path', '/user/elasticsearch/test/searchable_snapshots/simple'
  }

  // Only include the HA integration tests for the HA test task
//  integTestHa {
//    setIncludes(['**/Ha*TestSuiteIT.class'])
//  }
} else {
  if (legalPath) {
    logger.warn("hdfsFixture unsupported, please set HADOOP_HOME and put HADOOP_HOME\\bin in PATH")
  } else {
    logger.warn("hdfsFixture unsupported since there are spaces in the path: '" + rootProject.rootDir.toString() + "'")
  }

  // The normal integration test runner will just test that the plugin loads
  // TODO: Not doing rest tests
//  integTest {
//    systemProperty 'tests.rest.suite', 'hdfs_repository/10_basic'
//  }
  // HA fixture is unsupported. Don't run them.
//  integTestHa.setEnabled(false)
}

//check.dependsOn(integTestSecure, integTestSecureHa)
check.dependsOn(integTestSecure)

// Run just the secure hdfs rest test suite.
// TODO: Need the rest suite?
//integTestSecure {
//  systemProperty 'tests.rest.suite', 'secure_hdfs_repository'
//}
//// Ignore HA integration Tests. They are included below as part of integTestSecureHa test runner.
//integTestSecure {
//  exclude('**/Ha*TestSuiteIT.class')
//}
//// Only include the HA integration tests for the HA test task
//integTestSecureHa {
//  setIncludes(['**/Ha*TestSuiteIT.class'])
//}

//thirdPartyAudit {
//  ignoreMissingClasses()
//  ignoreViolations(
//    // internal java api: sun.misc.Unsafe
//    'com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator',
//    'com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator$1',
//    'org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm',
//    'org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot',
//  )
//}
