---
"line_26":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: GET
        path: "_analyze"
        body: |
          {
            "tokenizer": "keyword",
            "char_filter": [
              {
                "type": "mapping",
                "mappings": [
                  "٠ => 0",
                  "١ => 1",
                  "٢ => 2",
                  "٣ => 3",
                  "٤ => 4",
                  "٥ => 5",
                  "٦ => 6",
                  "٧ => 7",
                  "٨ => 8",
                  "٩ => 9"
                ]
              }
            ],
            "text": "My license plate is ٢٥٠١٥"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "My license plate is 25015",
              "start_offset": 0,
              "end_offset": 25,
              "type": "word",
              "position": 0
            }
          ]
        }
---
"line_109":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "my-index-000001"
        body: |
          {
            "settings": {
              "analysis": {
                "analyzer": {
                  "my_analyzer": {
                    "tokenizer": "standard",
                    "char_filter": [
                      "my_mappings_char_filter"
                    ]
                  }
                },
                "char_filter": {
                  "my_mappings_char_filter": {
                    "type": "mapping",
                    "mappings": [
                      ":) => _happy_",
                      ":( => _sad_"
                    ]
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: GET
        path: "my-index-000001/_analyze"
        body: |
          {
            "tokenizer": "keyword",
            "char_filter": [ "my_mappings_char_filter" ],
            "text": "I'm delighted about it :("
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "I'm delighted about it _sad_",
              "start_offset": 0,
              "end_offset": 25,
              "type": "word",
              "position": 0
            }
          ]
        }
