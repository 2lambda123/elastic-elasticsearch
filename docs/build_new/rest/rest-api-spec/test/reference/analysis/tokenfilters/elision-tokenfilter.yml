---
"line_34":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: GET
        path: "_analyze"
        body: |
          {
            "tokenizer" : "standard",
            "filter" : ["elision"],
            "text" : "j’examine près du wharf"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens" : [
            {
              "token" : "examine",
              "start_offset" : 0,
              "end_offset" : 9,
              "type" : "<ALPHANUM>",
              "position" : 0
            },
            {
              "token" : "près",
              "start_offset" : 10,
              "end_offset" : 14,
              "type" : "<ALPHANUM>",
              "position" : 1
            },
            {
              "token" : "du",
              "start_offset" : 15,
              "end_offset" : 17,
              "type" : "<ALPHANUM>",
              "position" : 2
            },
            {
              "token" : "wharf",
              "start_offset" : 18,
              "end_offset" : 23,
              "type" : "<ALPHANUM>",
              "position" : 3
            }
          ]
        }
---
"line_96":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "elision_example"
        body: |
          {
            "settings": {
              "analysis": {
                "analyzer": {
                  "whitespace_elision": {
                    "tokenizer": "whitespace",
                    "filter": [ "elision" ]
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
---
"line_165":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "elision_case_insensitive_example"
        body: |
          {
            "settings": {
              "analysis": {
                "analyzer": {
                  "default": {
                    "tokenizer": "whitespace",
                    "filter": [ "elision_case_insensitive" ]
                  }
                },
                "filter": {
                  "elision_case_insensitive": {
                    "type": "elision",
                    "articles": [ "l", "m", "t", "qu", "n", "s", "j" ],
                    "articles_case": true
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
