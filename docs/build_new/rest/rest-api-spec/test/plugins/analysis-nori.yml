---
"line_96":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "nori_sample"
        body: |
          {
            "settings": {
              "index": {
                "analysis": {
                  "tokenizer": {
                    "nori_user_dict": {
                      "type": "nori_tokenizer",
                      "decompound_mode": "mixed",
                      "discard_punctuation": "false",
                      "user_dictionary": "userdict_ko.txt"
                    }
                  },
                  "analyzer": {
                    "my_analyzer": {
                      "type": "custom",
                      "tokenizer": "nori_user_dict"
                    }
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: GET
        path: "nori_sample/_analyze"
        body: |
          {
            "analyzer": "my_analyzer",
            "text": "세종시"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens" : [ {
            "token" : "세종시",
            "start_offset" : 0,
            "end_offset" : 3,
            "type" : "word",
            "position" : 0,
            "positionLength" : 2
          }, {
            "token" : "세종",
            "start_offset" : 0,
            "end_offset" : 2,
            "type" : "word",
            "position" : 0
          }, {
            "token" : "시",
            "start_offset" : 2,
            "end_offset" : 3,
            "type" : "word",
            "position" : 1
           }]
        }
---
"line_169":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "nori_sample"
        body: |
          {
            "settings": {
              "index": {
                "analysis": {
                  "tokenizer": {
                    "nori_user_dict": {
                      "type": "nori_tokenizer",
                      "decompound_mode": "mixed",
                      "user_dictionary_rules": ["c++", "C쁠쁠", "세종", "세종시 세종 시"]
                    }
                  },
                  "analyzer": {
                    "my_analyzer": {
                      "type": "custom",
                      "tokenizer": "nori_user_dict"
                    }
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
---
"line_200":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: GET
        path: "_analyze"
        body: |
          {
            "tokenizer": "nori_tokenizer",
            "text": "뿌리가 깊은 나무는",
            "attributes" : ["posType", "leftPOS", "rightPOS", "morphemes", "reading"],
            "explain": true
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "detail": {
            "custom_analyzer": true,
            "charfilters": [],
            "tokenizer": {
              "name": "nori_tokenizer",
              "tokens": [
                {
                  "token": "뿌리",
                  "start_offset": 0,
                  "end_offset": 2,
                  "type": "word",
                  "position": 0,
                  "leftPOS": "NNG(General Noun)",
                  "morphemes": null,
                  "posType": "MORPHEME",
                  "reading": null,
                  "rightPOS": "NNG(General Noun)"
                },
                {
                  "token": "가",
                  "start_offset": 2,
                  "end_offset": 3,
                  "type": "word",
                  "position": 1,
                  "leftPOS": "J(Ending Particle)",
                  "morphemes": null,
                  "posType": "MORPHEME",
                  "reading": null,
                  "rightPOS": "J(Ending Particle)"
                },
                {
                  "token": "깊",
                  "start_offset": 4,
                  "end_offset": 5,
                  "type": "word",
                  "position": 2,
                  "leftPOS": "VA(Adjective)",
                  "morphemes": null,
                  "posType": "MORPHEME",
                  "reading": null,
                  "rightPOS": "VA(Adjective)"
                },
                {
                  "token": "은",
                  "start_offset": 5,
                  "end_offset": 6,
                  "type": "word",
                  "position": 3,
                  "leftPOS": "E(Verbal endings)",
                  "morphemes": null,
                  "posType": "MORPHEME",
                  "reading": null,
                  "rightPOS": "E(Verbal endings)"
                },
                {
                  "token": "나무",
                  "start_offset": 7,
                  "end_offset": 9,
                  "type": "word",
                  "position": 4,
                  "leftPOS": "NNG(General Noun)",
                  "morphemes": null,
                  "posType": "MORPHEME",
                  "reading": null,
                  "rightPOS": "NNG(General Noun)"
                },
                {
                  "token": "는",
                  "start_offset": 9,
                  "end_offset": 10,
                  "type": "word",
                  "position": 5,
                  "leftPOS": "J(Ending Particle)",
                  "morphemes": null,
                  "posType": "MORPHEME",
                  "reading": null,
                  "rightPOS": "J(Ending Particle)"
                }
              ]
            },
            "tokenfilters": []
          }
        }
---
"line_335":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "nori_sample"
        body: |
          {
            "settings": {
              "index": {
                "analysis": {
                  "analyzer": {
                    "my_analyzer": {
                      "tokenizer": "nori_tokenizer",
                      "filter": [
                        "my_posfilter"
                      ]
                    }
                  },
                  "filter": {
                    "my_posfilter": {
                      "type": "nori_part_of_speech",
                      "stoptags": [
                        "NR"
                      ]
                    }
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: GET
        path: "nori_sample/_analyze"
        body: |
          {
            "analyzer": "my_analyzer",
            "text": "여섯 용이"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens" : [ {
            "token" : "용",
            "start_offset" : 3,
            "end_offset" : 4,
            "type" : "word",
            "position" : 1
          }, {
            "token" : "이",
            "start_offset" : 4,
            "end_offset" : 5,
            "type" : "word",
            "position" : 2
          } ]
        }
---
"line_400":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "nori_sample"
        body: |
          {
            "settings": {
              "index": {
                "analysis": {
                  "analyzer": {
                    "my_analyzer": {
                      "tokenizer": "nori_tokenizer",
                      "filter": [ "nori_readingform" ]
                    }
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: GET
        path: "nori_sample/_analyze"
        body: |
          {
            "analyzer": "my_analyzer",
            "text": "鄕歌"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens" : [ {
            "token" : "향가",
            "start_offset" : 0,
            "end_offset" : 2,
            "type" : "word",
            "position" : 0
          }]
        }
---
"line_482":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "nori_sample"
        body: |
          {
            "settings": {
              "index": {
                "analysis": {
                  "analyzer": {
                    "my_analyzer": {
                      "tokenizer": "tokenizer_discard_puncuation_false",
                      "filter": [
                        "part_of_speech_stop_sp", "nori_number"
                      ]
                    }
                  },
                  "tokenizer": {
                    "tokenizer_discard_puncuation_false": {
                      "type": "nori_tokenizer",
                      "discard_punctuation": "false"
                    }
                  },
                  "filter": {
                      "part_of_speech_stop_sp": {
                          "type": "nori_part_of_speech",
                          "stoptags": ["SP"]
                      }
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: GET
        path: "nori_sample/_analyze"
        body: |
          {
            "analyzer": "my_analyzer",
            "text": "십만이천오백과 ３.２천"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens" : [{
            "token" : "102500",
            "start_offset" : 0,
            "end_offset" : 6,
            "type" : "word",
            "position" : 0
          }, {
            "token" : "과",
            "start_offset" : 6,
            "end_offset" : 7,
            "type" : "word",
            "position" : 1
          }, {
            "token" : "3200",
            "start_offset" : 8,
            "end_offset" : 12,
            "type" : "word",
            "position" : 2
          }]
        }
