[role="xpack"]
[testenv="platinum"]
[[xpack-ccr]]
== {ccr-cap}

Use {ccr} (CCR) to enable replication of indices in remote clusters to a
local cluster. {ccr-cap} enables several use cases within the Elastic Stack:

* *Disaster recovery* in case a primary cluster fails, with a secondary
cluster serving as a hot backup
* *Data locality* to serve reads locally and reduce costly latency
* *Centralized reporting* for preventing search load from interfering with indexing by offloading search to a secondary cluster

{ccr-cap} is done on an index-by-index basis, so replication is
configured at the index level. For each configured replica, there is a
replication source index called the _leader index_ and a replication target
index called the _follower index_.

The following points are important concepts regarding {ccr}:

* Replication is active-passive. While the leader index
can directly be written into, the follower index can not directly receive
writes.
* Replication is driven by the follower index. This model simplifies
state management on the leader index and means
that {ccr} does not interfere with indexing on the leader index.
* Replication is pull-based. The cluster performing this pull is called
the _local cluster_, and the cluster being replicated is called the
_remote cluster_.

[discrete]
[[ccr-version-compatibility]]
=== Version compatibility
The {es} version of the local cluster must be **the same as or newer** than
the remote cluster. If newer, the versions must also be compatible as outlined
in the following matrix.

[%collapsible]
.Version compatibility matrix
====
include::../modules/remote-clusters.asciidoc[tag=remote-cluster-compatibility-matrix]
====

[discrete]
[[ccr-replication-mechanics]]
=== Replication mechanics
Although you configure {ccr} replication at the index level, {es}
achieves replication at the shard level. When a follower index is created,
each shard in that index pulls changes form its corresponding shard in the
leader index, which means that a follower index has the same number of
shards as its leader index. All operations are replicated by the follower, so that operations to create, update, or delete a document are replicated.
These requests can be served from any copy of the leader shard (primary or
replica).

When a follower shard sends a read request, the leader shard responds with
any new operations, limited by the read parameters that you establish when
configuring the follower index. If no new operations are available, the
leader shard waits up to the configured timeout for new operations. If the
timeout elapses, the leader shard responds to the follower shard that there
are no new operations. The follower shard updates shard statistics and
immediately sends another read request to the leader shard. This
communication model ensures that network connections between the remote
cluster and the local cluster are continually in use, avoiding forceful
termination from an external sources such as a firewall.

If a read request fails, the cause of the failure is inspected. If the
cause of the failure is deemed to be recoverable (such as a network
failure), the follower shard enters into a retry loop. Otherwise, the
follower shard pauses until you resume it, either through {kib} or by
using the <<ccr-post-resume-follow,resume follower API>>.

When a follower shard receives operations from the leader shard, it places
those operations in a write buffer. The follower shard uses the write buffer
to submit bulk write requests within the shard. If the write buffer exceeds
its configured limits, no additional read requests are sent. This configuration
provides a back-pressure against read requests, allowing the follower shard
to resume sending read requests when the write buffer is no longer full.

To manage how operations are replicated from the leader index, you can
configure settings when creating the follower index in {kib}, or use the
<<ccr-put-follow,create follower API>>.

The follower index automatically retrieves some updates applied to the leader
index, while other updates are retrieved as needed:

[cols="3"]
|===
h| Update type h| Automatic  h| As needed
| Alias        | {yes-icon} | {no-icon}
| Mapping      | {no-icon}  | {yes-icon}
| Settings     | {no-icon}  | {yes-icon}
|===

The follower index automatically retrieves alias updates. Mapping updates and
settings updates are retrieved as needed by the follower index. For example,
changing the number of replicas on the leader index is not replicated by the
follower index, so that setting might not be retrieved.

NOTE: You cannot manually modify the mapping or alias of a follower index.

If you apply a non-dynamic settings change to the leader index that is
needed by the follower index, the follower index closes itself, applies the
settings update, and then re-opens itself. The follower index is unavailable
for reads and cannot replicate writes during this cycle.

[discrete]
[[ccr-remote-recovery]]
=== Remote recovery process
When you create a follower index, you cannot use it until it is fully
initialized. The _remote recovery_ process builds a new copy of a shard on a
follower node by copying data from the primary shard in the leader cluster.

{es} uses this remote recovery process to bootstrap a follower index using the
data from the leader index. This process provides the follower with a copy of
the current state of the leader index, even if a complete history of changes
is not available on the leader due to Lucene segment merging.

Remote recovery is a network intensive process that transfers all of the Lucene
segment files from the leader cluster to the follower cluster. The follower
requests that a recovery session be initiated on the primary shard in the
leader cluster. The follower then requests file chunks concurrently from the
leader. By default, the process concurrently requests five `1Mb` file
chunks. This default behavior is designed to support leader and follower
clusters with high network latency between them.

You can modify dynamic <<ccr-recovery-settings,remote recovery settings>> to
rate-limit the transmitted data and manage the resources consumed by remote
recoveries.

Use the <<cat-recovery,recovery API>> on the follower cluster to obtain
information about an in-progress remote recovery. Because {es} implements
remote recoveries using the <<snapshot-restore,snapshot and restore>>
infrastructure, running remote recoveries are labelled as type
`snapshot` in the recovery API.

[discrete]
[[ccr-leader-requirements]]
=== Requirements for leader indices
{ccr-cap} works by replaying the history of individual write
operations that were performed on the shards of the leader index. This means
that the history of these operations needs to be retained on the leader shards
so that they can be pulled by the follower shard tasks. The underlying
mechanism used to retain these operations is _soft deletes_.

A soft delete occurs whenever an existing document is deleted or updated. By
retaining these soft deletes up to configurable limits, the history of
operations can be retained on the leader shards and made available to the
follower shard tasks as it replays the history of operations.

Soft deletes must be enabled for indices that you want to use as leader
indices. Soft deletes are enabled by default on new indices created on
or after {es} 7.0.0.

// tag::ccr-existing-indices-tag[]
IMPORTANT: {ccr-cap} cannot be used on existing indices. If you have
existing data that you want to replicate from another cluster, you must
<<docs-reindex,reindex>> your data into a new index with soft deletes
enabled.

// end::ccr-existing-indices-tag[]

[discrete]
[[ccr-learn-more]]
=== Learn more
This following sections provide more information about how to configure
and use {ccr}:

* <<ccr-getting-started>>
* <<ccr-managing>>
* <<ccr-auto-follow>>
* <<ccr-tuning>>
* <<ccr-upgrading>>

include::getting-started.asciidoc[]
include::managing.asciidoc[]
include::auto-follow.asciidoc[]
include::tuning.asciidoc[]
include::upgrading.asciidoc[]
