[[troubleshooting-unbalanced-cluster]]
== Troubleshooting an unbalanced cluster

Since 8.6 Elasticsearch is balancing shards across data tier to achieve a good compromise between
* shard count
* disk usage
* ingest load forecast for write indices

Please note, elasticsearch does not take into account amount or complexity of search queries when rebalancing shards.

There is no guarantee that individual components are going to be evenly spread across the nodes.
This might appear as some of the nodes have less shards allocated or using less disk
as long as they are assigned with the shards with greater ingest load forecast.

You might get a detailed output of assigned workloads per node using <<cat-allocation,cat allocation command>>.

Please note that some operations such as node restarting or decommissioning or changing cluster allocation settings
are disruptive and might require multiple shards to move or rebalance.

Shard movement order is not deterministic and mostly determined by the source and target node readiness to move a shard.
While rebalancing is in progress some nodes might appear busier then others.

You might monitor rebalancing progress using <<get-desired-balance,desired balance api>>.

[source,console,id=get-desired-balance-request-example]
--------------------------------------------------
GET /_internal/desired_balance
--------------------------------------------------

The API returns the following response:

[source,js]
--------------------------------------------------
...
  "cluster_balance_stats" : {
    "shard_count": 42,                        <1>
    "undesired_shard_allocation_count": 3,    <2>
...
--------------------------------------------------

<1> The total number of shards in cluster.
<2> The number of shards that needs to be moved to finish balancing.

When shard is allocated to undesired node it utilizes the resources of the current node instead of target.
This might cause a hotspot (disk or CPU) when multiple shards are residing on the current node that have not been
moved to their corresponding targets yet.

You might find log entries such as
[source,console]
--------------------------------------------------
[{}] of assigned shards ({}/{}) are not on their desired nodes, which exceeds the warn threshold of [{}]
--------------------------------------------------
if cluster detects a significant amount (>= 10%) of shards need to be moved.
This is not concerning as long as the number of such shards is decreasing and this warning appears occasionally,
for example after rolling restarts or changing allocation settings.


If cluster is experiencing this warning for extended period of time, it is possible that the desired balance diverged
too far from the current state. Then it is possible to reset the desired balance using the following API call
and calculate a new one that would hopefully require less shard movements to get to balanced state.

[source,console,id=delete-desired-balance-request-example]
--------------------------------------------------
DELETE /_internal/desired_balance
--------------------------------------------------


If cluster is getting into this state repeatedly without node restarts nor changes in allocation settings,
it is possible to increase the <<shards-rebalancing-heuristics,cluster.routing.allocation.balance.threshold>> in order
to reduce the sensitivity of the algorithm that tries to level up the shard count and disk usage within the cluster.
