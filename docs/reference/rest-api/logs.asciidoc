[role="xpack"]
[[logs-api]]
== Log ingestion API

experimental::[]

Provides a simple JSON API to ingest log events into {es}.

[discrete]
[[logs-api-request]]
=== {api-request-title}

`POST /_logs`

`POST /_logs/<dataset>`

`POST /_logs/<dataset>/<namespace>`

[discrete]
[[logs-api-prereqs]]
=== {api-prereq-title}
* If the {es} {security-features} are enabled, you must have the `create`
<<privileges-list-indices,index privileges>> for the target data stream.
* Automatic data stream creation requires a matching index template with data
stream enabled. See <<set-up-a-data-stream>>.

[discrete]
[[logs-api-desc]]
=== {api-description-title}

Provides a way to ingest multiple log events into {es}, similar to the <<docs-bulk, Bulk API>>.

The log events are specified in the request body using a newline delimited JSON (NDJSON) structure.

The events are indexed into the `logs-<dataset>-<namespace>` <<data-streams, data stream>>,
according to the dataset and namespace parameters, which can be provided globally or on a per-event basis.

The endpoint is designed in a way that logs are never dropped, as long as the cluster has enough capacity.

If an error happens during ingestion, the logs are sent to the `logs-dlq-<namespace>` data stream that acts as a dead letter queue for failed events.
However, log ingestion should rarely fail as the default mappings for the `logs-*-*` data streams are designed to minimize mapping conflicts.

A couple of fields from the {ecs-ref}[Elastic Common Schema (ECS)] are indexed by default that are commonly used to search or filter logs.
All other fields are not indexed by default. But you can still use these fields in searches and aggregations using <<runtime, runtime fields>>. If there are custom fields that you frequently use in searches or aggregations, you can add them to the index template for your dataset (`logs-<dataset>-*`) so that the field will be indexed for new data.
This comes at the expense of a larger index size more processing at ingest time.

All fields, aside from the `@timestamp` field, are configured to <<ignore-malformed, ignore malformed>> values.
This means that if a log event contains a field whose type is incompatible with the type of the field that exists in the mapping,
{es} will ignore the field instead of rejecting the whole document.
For example, when a string is provided for a field that is mapped to integer.
Note that this currently doesn't apply for object/scalar mismatches, such as `"foo": "bar"` vs `"foo.bar": "baz"`.

[discrete]
[[logs-api-path-params]]
=== {api-path-parms-title}

`data_stream.dataset`::
  (Optional, string)
  Defaults to `generic`.
  Describes the ingested data and its structure.
  It is highly recommended to provide a value for this so that you can add structure to your logs after the fact.
  Example: `nginx.access`.

`data_stream.namespace`::
  (Optional, string)
  Defaults to `default`.
  A user-configurable arbitrary grouping, such as an environment (dev, prod, or qa), a team, or a strategic business unit.

[discrete]
[[logs-api-query-params]]
=== {api-query-parms-title}

Any provided query parameter will be added to each log line.
For example, `/_logs?service.name=myapp` will add `"service.name": "myapp"` to all logs.
[discrete]
[[logs-api-request-body]]
=== {api-request-body-title}
The request body contains a newline-delimited list of log events to ingest.
The individual events don't have any required fields and can contain arbitrary JSON content.

TIP: Use the {ecs-logging-ref}/intro.html[ECS logging libraries] to create JSON logs that work best with the log ingestion API.

While there is no required structure for logs, there are a couple of fields with special semantics:

`@timestamp`::
(Optional, string)
The timestamp of the log event.
If not provided, will be set to the current time.

`data_stream.dataset`::
(Optional, string)
Overrides the dataset path parameter.

`data_stream.namespace`::
(Optional, string)
Overrides the namespace path parameter.

`message`::
(Optional, string)
The log message.

`log.level`::
(Optional, string)
The severity of the log message.
Example: `INFO`.

`service.name`::
(Optional, string) The name of the service the log event relates to.
This facilitates correlation with APM Services.

`service.environment`::
(Optional, string) The name of the service environment the log event relates to.
This facilitates correlation with APM Services.

`trace.id`::
(Optional, string) The APM trace id that this log event relates to.
This facilitates correlation with APM traces.

`transaction.id`::
(Optional, string) The APM transaction id that this log event relates to.
This facilitates correlation with APM transactions.

`span.id`::
(Optional, string) The APM span id that this log event relates to.
This facilitates correlation with APM spans.

`error.id`::
(Optional, string) The APM error id that this log event relates to.
This facilitates correlation with APM errors.

`_metadata`::
(Optional, object)
Marks this line as a metadata line.
Provides metadata that will be merged into subsequent events.
If a metadata event is provided as the first line, the metadata is added to all logs events.
If a metadata event is provided after the first line, the metadata is added to all subsequent log events until another metadata event is provided.
This way you can easily add global metadata and send logs from multiple datasets in a single request, providing dataset-specific metadata.

Dotted field names are expanded to objects so that they can be used interchangeably with nested objects. For example, the following documents are treated equally: `{"log.level": "INFO"}`, `{"log": { "level": "INFO"} }`.

[discrete]
[[logs-api-response-body]]
==== {api-response-body-title}

The log API's response body is always empty.

Status

* 202 Accepted: The log events have been received and are processed in the background. They should be searchable after a short while.
* 500 Internal Server Error: There was an error while processing the log events. Some logs may have been lost.

[discrete]
[[logs-api-example]]
=== {api-examples-title}

Ingests a single log into the `logs-myapp-default` data stream.
Provides global metadata via query parameters.

[source,console]
------------------------------------------------------------
POST _logs/myapp?service.name=myapp
{"@timestamp":"2016-05-23T08:05:34.853Z", "message":"Hello World", "custom_field": "value"}
------------------------------------------------------------

Event though `custom_field` is not among the <<logs-api-request-body, list of fields that are indexed by default>>,
you can use it in searches and aggregations via <<dynamic-mapping-runtime-fields, dynamically mapped runtime fields>>.

[source,console]
------------------------------------------------------------
POST logs-myapp-default/_search?q=custom_field:value
------------------------------------------------------------

The API returns the following response:

[source,console-result]
----
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 1.0,
    "hits": [
      {
        "_index": ".ds-logs-foo-default-2016.05.23-000001",
        "_id": "FKgQT4IBWsM7OYMsIp0N",
        "_score": 1.0,
        "_source": {
          "@timestamp": "2016-05-23T08:05:34.853Z",
          "message": "Hello World",
          "custom_field": "value",
          "service": {
            "name": "myapp"
          },
          "data_stream": {
            "type": "logs",
            "dataset": "myapp"
            "namespace": "default",
          }
        }
      }
    ]
  }
}
----

'''

Ingests a single log into the `logs-myapp-default` data stream.
Provides global metadata via a metadata event.

[source,console]
------------------------------------------------------------
POST _logs/myapp
{"_metadata": {"service.name":"myapp"}}
{"@timestamp":"2016-05-23T08:05:34.853Z", "message":"Hello World"}
------------------------------------------------------------

'''

Ingests a two log events into the `logs-myapp-default` and `logs-my_other_app-default` data stream, respectively.
Provides metadata via local metadata events.

[source,console]
------------------------------------------------------------
POST _logs
{"_metadata": {}}
{"_metadata": {"data_stream.dataset":"myapp"}}
{"@timestamp":"2016-05-23T08:05:34.853Z", "message":"Hello app"}
{"_metadata": {"data_stream.dataset":"my_other_app"}}
{"@timestamp":"2016-05-23T08:05:34.853Z", "message":"Hello other app"}
------------------------------------------------------------
