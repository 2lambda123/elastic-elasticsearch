[[node-tool]]
== elasticsearch-node
[float]
=== Background

Sometimes {es} nodes are temporarily stopped, perhaps because of the need to
perform some maintenance activity or perhaps because of a hardware failure.
Once the temporary condition has been resolved you should restart the node and
it will rejoin the cluster and continue normally. Depending on your
configuration, your cluster may be able to remain completely available even
while one or more of its nodes are stopped.

Sometimes it might not be possible to restart a node after it has stopped. For
example, the node's host may suffer from a hardware problem that cannot be
repaired. If the cluster is still available then you can start up
a fresh node on another host and {es} will bring this node into the cluster in place
of the failed node.

Each node stores its data in the data directories defined by the
<<path-settings,`path.data` setting>>. This means that in a disaster you can
also restart a node by moving its data directories to another host, presuming
that those data directories can be recovered from the faulty host. Note that it
is not possible to restore the data directory from a backup because this will
lead to data corruption. Backups of an {es} cluster can only be taken using
<<modules-snapshots>>.

Elasticsearch <<modules-discovery-quorums,requires a response from a majority
of the master-eligible nodes>> in order to elect a master and to update the
cluster state. This means that if you have three master-eligible nodes then the
cluster will remain available even if one of them has failed. However if two of
the three master-eligible nodes fail then the cluster will be unavailable until
at least one of them is restarted.

In very rare circumstances it may not be possible to restart enough nodes to
restore the cluster's availability. If such a disaster occurs then you should
build a new cluster from a recent snapshot, and re-import any data that was
ingested since that snapshot was taken.

However, if the disaster is serious enough then it may not be possible to
recover from a recent snapshot either. Unfortunately in this case there is no
way forward that does not risk data loss, but it may be possible to use the
`elasticsearch-node` tool to unsafely bring the cluster back online.

This tool has two modes, depending on whether there are any master-eligible
nodes remaining or not:

* `elastisearch-node unsafe-bootstap` can be used if there is at least one
  remaining master-eligible node. It allows a remaining node to become the
  elected master node without needing a response from any other nodes.

* `elastisearch-node detach-cluster` can be used if there are no remaining
  master-eligible nodes. It allows you to detach any remaining data nodes from
  the old, failed, cluster so they can join a new cluster.

[float]
=== Unsafe bootstrap

If there is at least one remaining master-eligible node, but it is not possible
to restart a majority of them, then the `elasticsearch-node unsafe-bootstrap`
command will allow one of the remaining nodes to become the elected master
without needing a response from any other nodes. This can lead to arbitrary
data loss since the node in question may not hold the latest cluster metadata,
and this out-of-date metadata may make it impossible to use some or all of the
indices in the cluster.

[WARNING]
Execution of this command can lead to arbitrary data loss. Only run this tool
if you understand and accept the possible consequences and have exhausted all
other possibilities for recovery of your cluster.

The sequence of operations for using this tool are as follows:

1. Make sure you have really lost access to at least half of the
master-eligible nodes in the cluster, and they cannot be repaired or recovered
by moving their data paths to healthy hardware.
2. Stop **all** remaining master-eligible nodes.
3. Run `elasticsearch-node unsafe-bootstrap` on each remaining
master-eligible node but do not let the tool proceed with overriding node
metadata. Remember (term, version) on each of the nodes.
4. Compare (term, version) pairs using lexicographical order and pick
master-eligible node with the highest (term, version).
5. On this node, run the `elasticsearch-node unsafe-bootstrap` command as shown
below. Verify that the tool reported `Master node was successfully
bootstrapped`.
6. Start this node and verify that it is elected as the master node.
7. Start all other master-eligible nodes and verify that each one joins the
cluster.
8. Any running master-ineligible nodes will automatically join the
newly-elected master. Restart any previously-stopped nodes and verify that the
cluster is now fully-formed.
9. Investigate the data in the cluster to discover if any was lost during this
process.

[WARNING]
When you run the tool it will make sure that the node being bootstrapped is not
running. It is important that all other master-eligible nodes are also stopped
while this tool is running, but the tool does not check this.

[NOTE]
The message `Master node was successfully bootstrapped` does not mean that
there has been no data loss, it just means that tool was able to complete its
job.

Consider you cluster had 5 master-eligible nodes and you have permanently
lost 3 of them. So you have 2 nodes remaining.

* Run the tool on the 1st survived node.
[source,txt]
----
node_1$ ./bin/elasticsearch-node unsafe-bootstrap

    WARNING: Elasticsearch MUST be stopped before running this tool.

Current node cluster state (term, version) pair is (1, 12)

You should run this tool only if you have permanently lost half or more
of the master-eligible nodes, and you cannot restore the cluster from
a snapshot.
This tool can cause arbitrary data loss and its use should be your last
resort.
If you have multiple survived master eligible nodes, consider running
this tool on the node with the highest cluster state (term, version)
pair.
Do you want to proceed?

Confirm [y/N] n
----
* Run the tool on the 2nd survived node.

[source,txt]
----
node_2$ ./bin/elasticsearch-node unsafe-bootstrap

    WARNING: Elasticsearch MUST be stopped before running this tool.

Current node cluster state (term, version) pair is (2, 3)

You should run this tool only if you have permanently lost half or more
of the master-eligible nodes, and you cannot restore the cluster from
a snapshot.
This tool can cause arbitrary data loss and its use should be your last
resort.
If you have multiple survived master eligible nodes, consider running
this tool on the node with the highest cluster state (term, version)
pair.
Do you want to proceed?

Confirm [y/N] n
----
* Now you know that (term, version) pair on 1st node is (1, 12) and on the
2nd node is (2, 3). It means that 2nd node has a latter cluster state and
it's better to bootstrap using node 2.

[source,txt]
----
node_2$ ./bin/elasticsearch-node unsafe-bootstrap

    WARNING: Elasticsearch MUST be stopped before running this tool.

Current node cluster state (term, version) pair is (2, 3)

You should run this tool only if you have permanently lost half or more
of the master-eligible nodes, and you cannot restore the cluster from
a snapshot.
This tool can cause arbitrary data loss and its use should be your last
resort.
If you have multiple survived master eligible nodes, consider running
this tool on the node with the highest cluster state (term, version)
pair.
Do you want to proceed?

Confirm [y/N] y
Master node was successfully bootstrapped
----

[float]
=== Detach cluster
Sample tool output

[source, txt]
----
node_3$ ./bin/elasticsearch-node detach-cluster

    WARNING: Elasticsearch MUST be stopped before running this tool.

You should run this tool only if you have permanently lost all your
master-eligible nodes and you cannot restore the cluster from
a snapshot or you have already run `elasticsearch-node unsafe-bootstrap`
on a master-eligible node that formed a cluster with this node.
This tool can cause arbitrary data loss and its use should be your last
resort.
Do you want to proceed?

Confirm [y/N] y
Node was successfully detached from the cluster
----


