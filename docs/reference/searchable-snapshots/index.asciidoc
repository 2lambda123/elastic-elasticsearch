[[searchable-snapshots]]
== {search-snaps-cap}

{search-snaps-cap} let you use <<snapshot-restore,snapshots>> to search and
store infrequently-accessed, read-only data. The <<cold-tier,cold>> and
<<frozen-tier,frozen>> data tiers use {search-snaps} to reduce your storage and
operating costs.

{search-snaps-cap} eliminate the need for <<scalability,replica shards>>,
potentially halving the local storage needed to search your data.
{search-snaps-cap} rely on the same snapshot mechanism you already use for
backups and have minimal impact on your snapshot repository storage costs.

[discrete]
[[mount-options]]
=== Mount options

To search a snapshot, you must first mount it locally as an index. There are two
mounting options, each with a different local storage footprint:

[[full-copy]]
Full copy::
Loads a full, local copy of the snapshotted index's shards into the cluster. The
cold tier uses this option by default.
+
If a node fails, {es} automatically recovers the index's shards from the
snapshot repository. Search performance for a full copy is comparable to a
regular index, with minimal need to access the snapshot repository.

[[shared-cache]]
Shared cache::
Loads only a local cache of the snapshotted index's data. This cache is shared
across the cluster's nodes. The frozen tier uses this option by default.
+
If a search requires data that's not in the cache, {es} lazily fetches the data
as needed from the snapshot repository. Searches that require these fetches are
slower, but repeated searches are served from the cache.
+
Although slower than a full local copy or a regular index, the shared snapshot
cache still returns results quickly, even for large data sets. This option
decouples compute and storage, letting you run searches with minimal compute
resources.

[discrete]
[[using-searchable-snapshots]]
=== Using {search-snaps}

Searching a {search-snap} index is the same as searching any other index.

By default, {search-snap} indices have no replicas. The underlying snapshot
provides resilience and the query volume is expected to be low enough that a
single shard copy will be sufficient. However, if you need to support a higher
query volume, you can add replicas by adjusting the `index.number_of_replicas`
index setting.

If a node fails and {search-snap} shards need to be restored from the snapshot,
there is a brief window of time while {es} allocates the shards to other nodes
where the cluster health will not be `green`. Searches that hit these shards
will fail or return partial results until the shards are reallocated to healthy
nodes.

You typically manage {search-snaps} through {ilm-init}. The
<<ilm-searchable-snapshot, searchable snapshots>> action automatically converts
a regular index into a {search-snap} index when it reaches the `cold` or
`frozen` phase. You can also make indices in existing snapshots searchable by
manually mounting them as {search-snap} indices with the
<<searchable-snapshots-api-mount-snapshot, mount snapshot>> API.

To mount an index from a snapshot that contains multiple indices, we recommend
creating a <<clone-snapshot-api, clone>> of the snapshot that contains only the
index you want to search, and mounting the clone. You should not delete a
snapshot if it has any mounted indices, so creating a clone enables you to
manage the lifecycle of the backup snapshot independently of any
{search-snaps}.

You can control the allocation of the shards of {search-snap} indices using the
same mechanisms as for regular indices. For example, you could use
<<shard-allocation-filtering>> to restrict {search-snap} shards to a subset of
your nodes.

We recommend that you <<indices-forcemerge, force-merge>> indices to a single
segment per shard before taking a snapshot that will be mounted as a
{search-snap} index. Each read from a snapshot repository takes time and costs
money, and the fewer segments there are the fewer reads are needed to restore
the snapshot.

[TIP]
====
{search-snaps-cap} are ideal for managing a large archive of historical data.
Historical information is typically searched less frequently than recent data
and therefore may not need replicas for their performance benefits.

For more complex or time-consuming searches, you can use <<async-search>> with
{search-snaps}.
====

[[searchable-snapshots-repository-types]]
You can use any of the following repository types with searchable snapshots:

* {plugins}/repository-s3.html[AWS S3]
* {plugins}/repository-gcs.html[Google Cloud Storage]
* {plugins}/repository-azure.html[Azure Blob Storage]
* {plugins}/repository-hdfs.html[Hadoop Distributed File Store (HDFS)]
* <<snapshots-filesystem-repository,Shared filesystems>> such as NFS

You can also use alternative implementations of these repository types, for
instance
{plugins}/repository-s3-client.html#repository-s3-compatible-services[Minio],
as long as they are fully compatible.

[discrete]
[[how-searchable-snapshots-work]]
=== How {search-snaps} work

When an index is mounted from a snapshot, {es} allocates its shards to data
nodes within the cluster. The data nodes then automatically retrieve the
relevant shard data from the repository onto local storage, based on the mount
options specified. Searches use the data held in local storage and, if not
locally available, download the necessary data from the repository. This avoids
incurring the cost or performance penalty associated with reading data from the
repository when the data is locally available.

If a node holding one of these shards fails, {es} automatically allocates it to
another node, and that node restores the relevant shard data again from the
repository. No replicas are needed, and no complicated monitoring or orchestration
is necessary to restore lost shards.

In the "full copy" mode, {es} restores a full copy of the {search-snap} shards
in the background and you can search them even if they have not been fully
restored just yet. If a search hits such a {search-snap} shard before it has been
fully restored, {es} eagerly retrieves the data needed to complete the search.

In the "shared cache" mode, {es} only stores small parts of the data locally on
a node in the cluster. This node-local shared cache has a fixed size and evicts
mapped file parts based on a "least-frequently-used" policy.

Replicas of {search-snaps} shards are recovered by accessing the data from the
snapshot repository. In contrast, replicas of regular indices are restored by
copying data from the primary.

[discrete]
[[back-up-restore-searchable-snapshots]]
=== Back up and restore {search-snaps}

You can use <<snapshot-lifecycle-management,regular snapshots>> to back up a
cluster containing {search-snap} indices. When you restore a snapshot
containing {search-snap} indices, these indices are restored as {search-snap}
indices again.

Before you restore a snapshot containing a {search-snap} index, you must first
<<snapshots-register-repository,register the repository>> containing the
original index snapshot. When restored, the {search-snap} index mounts the
original index snapshot from its original repository. If wanted, you
can use separate repositories for regular snapshots and {search-snaps}.

A snapshot of a {search-snap} index contains only a small amount of metadata
which identifies its original index snapshot. It does not contain any data from
the original index. The restore of a backup will fail to restore any
{search-snap} indices whose original index snapshot is unavailable.

[discrete]
[[searchable-snapshots-reliability]]
=== Reliability of {search-snaps}

The sole copy of the data in a {search-snap} index is the underlying snapshot,
stored in the repository. If the repository fails or corrupts the contents of
the snapshot then the data is lost. Although {es} may have made copies of the
data onto local storage, these copies may be incomplete and cannot be used to
recover any data after a repository failure. You must make sure that your
repository is reliable and protects against corruption of your data while it is
at rest in the repository.

The blob storage offered by all major public cloud providers typically offers
very good protection against data loss or corruption. If you manage your own
repository storage then you are responsible for its reliability.

[discrete]
[[searchable-snapshots-shared-cache]]
=== Shared snapshot cache

experimental::[]

Using {search-snap} in the "shared cache" mode, where only parts of the
data are locally cached on the nodes in the cluster, requires configuring a
shared snapshot cache which is used to hold a copy of just the
frequently-accessed parts of shards of indices which are mounted with
`?storage=shared_cache`. The `shared_cache` storage option is for example used
by the <<ilm-searchable-snapshot,ILM searchable snapshot action>> in the
<<frozen-tier,frozen tier>>.

If you configure a node to have a shared cache (disabled by default) then
that node will fully reserve the specified amount of space for the cache at
start up. Indices mounted with the `shared_cache` option can only be allocated
to nodes that have the shared cache explicitly configured.

`xpack.searchable.snapshot.shared_cache.size`::
(<<static-cluster-setting,Static>>, <<byte-units,byte value>>)
The size of the space reserved for the shared cache. Defaults to `0b`, meaning
that the node has no shared cache.

A reasonable value for the shared cache is anything between a couple of
gigabytes up to 90% of available disk space, if the node is to be exclusively
used for indices mounted with the `shared_cache` option.

NOTE: The shared cache can currently be configured on any node. In the future
this will be restricted to nodes having the <<frozen-tier,frozen tier>> data
role.

[discrete]
[[searchable-snapshots-shared-cache-ex]]
==== Example

Configuring a shared cache that can hold up to 4 terabytes of data is done by
adding the following line to your `elasticsearch.yml` file:

[source,yaml]
--------------------------------------------------
xpack.searchable.snapshot.shared_cache.size: "4TB"
--------------------------------------------------

[discrete]
[[searchable-snapshots-frozen-tier-on-cloud]]
==== Configuring the shared snapshot cache for the frozen tier on {ess}

On {ess}, the frozen tier is not fully integrated yet and requires a simple
manual configuration step.

Users in {ess} will have to chose one of the existing tiers in Cloud
(hot/warm/cold) to run the frozen tier functionality on. This can be configured
by link:{cloud}/ec-add-user-settings.html[adding the `xpack.searchable.snapshot.shared_cache.size` user setting]
to one of the existing tiers in the Elasticsearch Service Console.

Depending on whether the hot/warm/cold tier is to be exclusively used for the
new frozen functionality or whether it is to be shared with other data
on that tier, the shared_cache.size can be configured from just a few
gigabytes up to 90% of the available disk space.
