[[release-highlights-7.0.0]]
== 7.0.0 release highlights
++++
<titleabbrev>7.0.0</titleabbrev>
++++

See also <<breaking-changes-7.0,Breaking changes>> and
<<release-notes-7.0.0-alpha1,Release notes>>.

//NOTE: The notable-highlights tagged regions are re-used in the
//Installation and Upgrade Guide

//tag::notable-highlights[]
=== Adaptive replica selection enabled by default

In Elasticsearch 6.x and prior, a series of search requests to the same shard
would be forwarded to the primary and each replica in round-robin fashion. This
could prove problematic if one node starts a long garbage collection --- search
requests could still be forwarded to the slow node regardless and would have an
impact on search latency.

In 6.1, we added an experimental <<search-adaptive-replica,adaptive replica
selection>> feature. Each node tracks and compares how long search requests to
other nodes take, and uses this information to adjust how frequently to send
requests to shards on particular nodes. In our benchmarks, this results in an
overall improvement in search throughput and reduced 99th percentile latencies.

This option was disabled by default throughout 6.x, but we’ve heard feedback
from our users that have found the setting to be very beneficial, so we’ve
turned it on by default starting in Elasticsearch 7.0.0.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Skip shard refreshes if a shard is "search idle"

Elasticsearch 6.x and prior <<indices-refresh,refreshed>> indices automatically
in the background, by default every second.  This provides the “near real-time”
search capabilities Elasticsearch is known for: results are available for search
requests within one second after they'd been added, by default. However, this
behavior has a significant impact on indexing performance if the refreshes are
not needed, (e.g., if Elasticsearch isn’t servicing any active searches).

Elasticsearch 7.0 is much smarter about this behavior by introducing the
notion of a shard being "search idle". A shard now transitions to being search
idle after it hasn't had any searches for <<dynamic-index-settings,thirty
seconds>>, by default. Once a shard is search idle, all scheduled refreshes will
be skipped until a search comes through, which will trigger the next scheduled
refresh. We know that this is going to significantly increase the indexing
throughput for many users. The new behavior is only applied if there is no
explicit <<dynamic-index-settings,refresh interval set>>, so do set the refresh
interval explicitly for any indices on which you prefer the old behavior.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Default to one shard

One of the biggest sources of troubles we’ve seen over the years from our users
has been over-sharding and defaults play a big role in that. In Elasticsearch
6.x and prior, we defaulted to five shards by default per index. If you had one
daily index for ten different applications and each had the default of five
shards, you were creating fifty shards per day and it wasn't long before you had
thousands of shards even if you were only indexing a few gigabytes of data per
day. Index Lifecycle Management was a first step to help with this: providing
native rollover functions to create indexes by size instead of (just) by day and
built-in shrink functionality to shrink the number of shards per
index. Defaulting indices to one shard is the next step in helping to reduce
over-sharding. Of course, if you have another preferred primary shard count, you
can set it via the index settings.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Lucene 8

As with every major release, we look to support the latest major version of
Lucene, along with all the goodness that comes with it. That includes all the
developments that we contributed to the new Lucene version. Elasticsearch 7.0
bundles Lucene 8, which is the latest version of Lucene. Lucene version 8 serves
as the foundation for many functional improvements in the rest of Elasticsearch,
including improved search performance for top-k queries and better ways to
combine relevance signals for your searches while still maintaining speed.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Introduce the ability to minimize round-trips in {ccs}

In Elasticsearch 5.3, we released a feature called
<<modules-cross-cluster-search,{ccs}>> for users to query across multiple
clusters. We’ve since improved on the {ccs} framework, adding features to
ultimately use it to deprecate and replace tribe nodes as a way to federate
queries. In Elasticsearch 7.0, we’re adding a new execution mode for {ccs}: one
which has fewer round-trips when they aren't necessary. This mode
(`ccs_minimize_roundtrips`) can result in faster searches when the {ccs} query
spans high-latencies (e.g., across a WAN).
//end::notable-highlights[]

//tag::notable-highlights[]
=== New cluster coordination implementation

Since the beginning, we focused on making Elasticsearch easy to scale and
resilient to catastrophic failures. To support these requirements, we created a
pluggable cluster coordination system, with the default implementation known as
Zen Discovery. Zen Discovery was meant to be effortless, and give our users
peace of mind (as the name implies). The meteoric rise in Elasticsearch usage
has taught us a great deal. For instance, Zen's `minimum_master_nodes` setting
was often misconfigured, which put clusters at a greater risk of split brains
and losing data. Maintaining this setting across large and dynamically resizing
clusters was also difficult.

In Elasticsearch 7.0, we have completely rethought and rebuilt the cluster
coordination layer. The new implementation gives safe sub-second master election
times, where Zen may have taken several seconds to elect a new master, valuable
time for a mission-critical deployment. With the `minimum_master_nodes` setting
removed, growing and shrinking clusters becomes safer and easier, and leaves
much less room to misconfigure the system. Most importantly, the new cluster
coordination layer gives us strong building blocks for the future of
Elasticsearch, ensuring we can build functionality for even more advanced
use-cases to come.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Better support for small heaps (the real-memory circuit breaker)

Elasticsearch 7.0 adds an all-new <<circuit-breaker,circuit breaker>> that keeps
track of the total memory used by the JVM and will reject requests if they would
cause the reserved plus actual heap usage to exceed 95%. We'll also be changing
the default maximum buckets to return as part of an aggregation
(`search.max_buckets`) to 10,000, which is unbounded by default in 6.x and
prior. These two show great signs at seriously improving the out-of-memory
protection of Elasticsearch in 7.x, helping you keep your cluster alive even in
the face of adversarial or novice users running large queries and aggregations.
//end::notable-highlights[]

//tag::notable-highlights[]
=== {ccr-cap} is production-ready

We introduced {ccr-cap} as a beta feature in Elasticsearch
6.5. {ccr-cap} was the most heavily requested features for Elasticsearch. We're
excited to announce {ccr-cap} is now generally available and ready for production use
in Elasticsearch 6.7 and 7.0! {ccr-cap} has a variety of use cases, including
cross-datacenter and cross-region replication, replicating data to get closer to
the application server and user, and maintaining a centralized reporting cluster
replicated from a large number of smaller clusters.

In addition to maturing to a GA feature, there were a number of important
technical advancements in CCR for 6.7 and 7.0. Previous versions of {ccr-cap} required
replication to start on new indices only: existing indices could not be
replicated. {ccr-cap} can now start replicating existing indices that have soft
deletes enabled in 6.7 and 7.0, and new indices default to having soft deletes
enabled. We also introduced new technology to prevent a follower index from
falling fatally far behind its leader index. We’ve added a management UI in
Kibana for configuring remote clusters, indices to replicate, and index naming
patterns for automatic replication (e.g. for replicating `metricbeat-*`
indices). We've also added a monitoring UI for insight into {ccr} progress and
alerting on errors. Check out the Getting started with {ccr}
guide, or visit the reference documentation to learn more.
//end::notable-highlights[]

//tag::notable-highlights[]
=== {ilm} is production-ready

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== SQL is production-ready

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== High-level REST client is feature-complete

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Support nanosecond timestamps

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Faster retrieval of top hits

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Support for TLS 1.3

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Bundle JDK in Elasticsearch distribution

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Removal of types

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== Rank feature fields

Words.
//end::notable-highlights[]

//tag::notable-highlights[]
=== JSON logging

Words.
//end::notable-highlights[]
