[[restart-upgrade]]
=== Full cluster restart upgrade

A full cluster restart upgrade requires that you shut all nodes in the cluster
down, upgrade them, and restart the cluster. A full cluster restart was required
when upgrading to major versions prior to 6.x. Elasticsearch 6.x supports
<<rolling-upgrades, rolling upgrades>> from *Elasticsearch 5.6*. Upgrading to
6.x from earlier versions requires a full cluster restart. See the
<<upgrade-paths,Upgrade paths table>> to verify the type of upgrade you need
to perform.

To perform a full cluster restart upgrade:

. *Disable shard allocation.*
+
--

When you shut down a node, the allocation process waits for one minute
before starting to replicate the shards on that node to other nodes
in the cluster, causing a lot of wasted I/O. This can be avoided by disabling
allocation before shutting down the node:

[source,js]
--------------------------------------------------
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}
--------------------------------------------------
// CONSOLE
// TEST[skip:indexes don't assign]
--

. *Stop indexing and perform a synced flush.*
+
--

Performing a <<indices-synced-flush, synced-flush>> speeds up shard
recovery.

[source,sh]
--------------------------------------------------
POST _flush/synced
--------------------------------------------------
// CONSOLE

Synced flush operations are performed on a "best effort" basis. The request
fails if there are any pending indexing operations, but it is safe to
reissue the request multiple times until it succeeds.
--

. *Shutdown all nodes.*
+
--
* If you are running Elasticsearch with `systemd`:
+
[source,sh]
--------------------------------------------------
sudo systemctl stop elasticsearch.service
--------------------------------------------------

* If you are running Elasticsearch with SysV `init`:
+
[source,sh]
--------------------------------------------------
sudo -i service elasticsearch stop
--------------------------------------------------

* If you are running Elasticsearch as a daemon:
+
[source,sh]
--------------------------------------------------
kill `cat pid`
--------------------------------------------------
--

. *Upgrade all nodes.*
+
--
To upgrade using a <<deb,Debian>> or <<rpm,RPM>> package:

*   Use `rpm` or `dpkg` to install the new package.  All files are
    installed in the appropriate location for the operating system
    and Elasticsearch config files are not overwritten.

To upgrade using a zip or compressed tarball:

.. Extract the zip or tarball to a _new_ directory to be sure that you don't
    overwrite the `config` or `data` directories.

.. Either copy the files in the `config` directory from your old installation
    to your new installation, or set the environment variable `ES_JVM_OPTIONS`
    to the location of the `jvm.options` file and use the `-E path.conf=`
    option on the command line to point to your external config directory.

.. Either copy the files in the `data` directory from your old installation
    to your new installation, or set `path.data` in `config/elasticsearch.yml`
    to point to your external data directory.

[TIP]
================================================

When using the zip or tarball packages, the `config`, `data`, `logs` and
`plugins` directories are placed within the Elasticsearch home directory by
default.

We recommend putting these directories in a different location so that
there is no chance of deleting them when upgrading Elasticsearch.  These
custom paths can be <<path-settings,configured>> by setting `path.conf`,
`path.logs`, and `path.data` in `config/elasticsearch.yml` and using
`ES_JVM_OPTIONS` to specify the location of the `jvm.options` file.

The <<deb,Debian>> and <<rpm,RPM>> packages place these directories in the
appropriate place for each operating system.

================================================
--

. *Upgrade any plugins.*
+
Use the `elasticsearch-plugin` script to install the upgraded version of each
installed Elasticsearch plugin. When you upgrade a node, you must also upgrade
its plugins.

. *Start each upgraded node.*
+
--
If you have dedicated master nodes, start them first and wait for them to
form a cluster and elect a master before proceeding with your data nodes.
You can check progress by looking at the logs.

As soon as the <<master-election,minimum number of master-eligible nodes>>
have discovered each other, they form a cluster and elect a master. At
that point, you can use <<cat-health,`_cat/health`>> and
<<cat-nodes,`_cat/nodes`>> to monitor nodes joining the cluster:

[source,sh]
--------------------------------------------------
GET _cat/health

GET _cat/nodes
--------------------------------------------------
// CONSOLE

The `status` column returned by `_cat/health` shows the health of each node
in the cluster: `red`, `yellow`, or `green`.
--

. *Wait for all nodes to join the cluster and report a status of yellow.*
+
--
When a node joins the cluster, it begins to recover any primary shards that
are stored locally. The <<cat-health,`_cat/health`>> API initially reports
a `status` of `red`, indicating that not all primary shards have been allocated.

Once a node recovers its local shards, its `status` switches to  `yellow`,
indicating that all primary shards have been recovered, but not all replica
shards are allocated. This is to be expected because you have not yet
reenabled allocation. Delaying the allocation of replicas until all nodes
are `yellow` allows the master to allocate replicas to nodes that
already have local shard copies.
--

. *Reenable allocation.*
+
--
When all nodes have joined the cluster and recovered their primary shards,
reenable allocation.

[source,js]
------------------------------------------------------
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}
------------------------------------------------------
// CONSOLE

Once allocation is reenabled, the cluster starts allocating replica shards to
the data nodes. At this point it is safe to resume indexing and searching,
but your cluster will recover more quickly if you can wait until all primary
and replica shards have been successfully allocated and the status of all nodes
is `green`.

You can monitor progress with the <<cat-health,`_cat/health`>> and
<<cat-recovery,`_cat/recovery`>> APIs:

[source,sh]
--------------------------------------------------
GET _cat/health

GET _cat/recovery
--------------------------------------------------
// CONSOLE
--
