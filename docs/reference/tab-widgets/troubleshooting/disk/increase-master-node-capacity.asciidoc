// tag::cloud[]

. Log in to the {ess-console}[{ecloud} console].
+
. On the **Elasticsearch Service** panel, click the gear under the `Manage deployment` column that corresponds to the
name of your deployment.
+
. Go to `Actions > Edit deployment`, and go to the `Master instance` section:
+
[role="screenshot"]
image::images/troubleshooting/disk/increase-disk-capacity-master-node.png[Increase disk capacity of master nodes,align="center"]

. Choose larger capacity from the drop-down and click `save`. Wait for the plan to be applied and the problem
should be resolved.

// end::cloud[]

// tag::self-managed[]
In order to increase the master node capacity in your cluster, you will need to replace the current master nodes with
master nodes of higher capacity.

. First, we need to retrieve the disk threshold that will indicate how much space we need. The relevant thresholds is
the <<cluster-routing-watermark-high, high watermark>>. We can retrieve that via the following command:
+
[source,console]
----
GET _cluster/settings?include_defaults&filter_path=*.cluster.routing.allocation.disk.watermark.high*
----
+
The response will look like this:
+
[source,console-result]
----
{
  "defaults": {
    "cluster": {
      "routing": {
        "allocation": {
          "disk": {
            "watermark": {
              "high": "90%",                                <1>
              "high.max_headroom": "150GB"                  <2>
            }
          }
        }
      }
    }
  }
----
// TEST[skip:illustration purposes only]
+
The above means that in order to resolve the disk shortage we need at either drop our disk usage below the 90% or have
more than 150GB available.

. The next step is to find out our current disk usage, this way we can calculate how much space we need. For simplicity,
we show below only the master nodes.
+
[source,console]
----
GET /_cat/nodes?v&h=name,master,node.role,disk.used_percent,disk.used,disk.avail,disk.total
----
+
The response will look like this:
+
[source,console-result]
----
name                master node.role disk.used_percent disk.used disk.avail disk.total
instance-0000000000 *      m                    85.31    3.4gb     500mb       4gb
instance-0000000001 *      m                    50.02    2.1gb     1.9gb       4gb
instance-0000000002 *      m                    50.02    1.9gb     2.1gb       4gb
----
// TEST[skip:illustration purposes only]

. The desirable situation is to drop the disk usages bellow the relevant threshold, in our example 90%. Calculate
how much you need for this and allow for some padding, so it will not go over the threshold soon. Add this to the
current total disk space. This is how large the disk capacity of your master node should be.

. If you have multiple master nodes you need ensure that all master nodes will have this capacity. Assuming you have
the new nodes ready. Follow the next steps for every existing master node.

. Bring down one of the master nodes.
. Start up one of the new master nodes and wait for it to join the cluster. You can check this via:
[source,console]
----
GET /_cat/nodes?v&h=name,master,node.role,disk.used_percent,disk.used,disk.avail,disk.total
----
. Only after you have confirmed that your cluster has the initial number of master nodes, move forward to the next one.
// end::self-managed[]
