[[analysis-multiplexer-tokenfilter]]
=== Multiplexer Token Filter

A token filter of type `multiplexer` will emit multiple tokens at the same position,
each version of the token having been run through a different filter.

Note that the child filters will in effect be passed a mock tokenstream consisting
of a single token, so filters that read ahead in the tokenstream like shingles or
multi-word synonyms will not work properly

[float]
=== Options
[horizontal]
filters:: a list of token filters to apply to incoming tokens.  These can be any
  token filters defined elsewhere in the index mappings.  Filters can be chained
  using a comma-delimited string, so for example `"lowercase, porter_stem"` would
  apply the `lowercase` filter and then the `porter_stem` filter to a single token
preserve_original:: if `true` (the default) then emit the original token in
  addition to the filtered tokens

[float]
=== Settings example

You can set it up like:

[source,js]
--------------------------------------------------
PUT /multiplexer_example
{
    "settings" : {
        "analysis" : {
            "analyzer" : {
                "my_analyzer" : {
                    "tokenizer" : "standard",
                    "filter" : [ "my_multiplexer" ]
                }
            },
            "filter" : {
                "my_multiplexer" : {
                    "type" : "multiplexer",
                    "filters" : [ "lowercase", "lowercase, porter_stem" ]
                }
            }
        }
    }
}
--------------------------------------------------
// CONSOLE

And test it like:

[source,js]
--------------------------------------------------
POST /multiplexer_example/_analyze
{
  "analyzer" : "my_analyzer",
  "text" : "Going HOME"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

And it'd respond:

[source,js]
--------------------------------------------------
{
  "tokens": [
    {
      "token": "Going",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<WORD>",
      "position": 1
    },
    {
      "token": "going",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<WORD>",
      "position": 1
    },
    {
      "token": "go",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<WORD>",
      "position": 1
    },
    {
      "token": "HOME",
      "start_offset": 6,
      "end_offset": 10,
      "type": "<WORD>",
      "position": 2
    },
    {
      "token": "home",
      "start_offset": 6,
      "end_offset": 10,
      "type": "<WORD>",
      "position": 2
    },
    {
      "token": "home",
      "start_offset": 6,
      "end_offset": 10,
      "type": "<WORD>",
      "position": 2
    }
  ]
}
--------------------------------------------------
// TESTRESPONSE