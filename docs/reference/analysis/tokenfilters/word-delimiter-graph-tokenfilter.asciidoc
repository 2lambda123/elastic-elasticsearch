[[analysis-word-delimiter-graph-tokenfilter]]
=== Word delimiter graph token filter
++++
<titleabbrev>Word delimiter graph</titleabbrev>
++++

Splits tokens at non-alphanumeric characters. The `word_delimiter_graph` filter
also performs optional token normalization based on a set of rules. By default,
the filter uses the following rules:

* Split tokens at non-alphanumeric characters.
  The filter uses these characters as delimiters.
  For example: `Wi-Fi` -> `Wi`, `Fi`
* Remove leading or trailing delimiters from each token.
  For example: `hello---there, 'dude'` -> `hello`, `there`, `dude`
* Split tokens at letter case transitions.
  For example: `PowerShot` -> `Power`, `Shot`
* Split tokens at letter-number transitions.
  For example: `SD500` -> `SD`, `500`
* Remove the English possessive (`'s`) from the end of each token.
  For example: `Neil's` -> `Neil`

The `word_delimiter_graph` filter uses Lucene's
{lucene-analysis-docs}/miscellaneous/WordDelimiterGraphFilter.html[WordDelimiterGraphFilter]

[[analysis-word-delimiter-graph-tokenfilter-analyze-ex]]
==== Example

The following analyze API request uses the `word_delimiter_graph` filter to
split `Neil's Wi-Fi-enabled PowerShot SD500` into normalized tokens using the
filter's default rules:

[source,console]
----
GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [ "word_delimiter_graph" ],
  "text": "Neil's Wi-Fi-enabled PowerShot SD500"
}
----

The filter produces the following tokens:

[source,txt]
----
[ Neil, Wi, Fi, enabled, Power, Shot, SD, 500 ]
----

////
[source,console-result]
----
{
  "tokens" : [
    {
      "token" : "Neil",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "Wi",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "Fi",
      "start_offset" : 10,
      "end_offset" : 12,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "enabled",
      "start_offset" : 13,
      "end_offset" : 20,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "Power",
      "start_offset" : 21,
      "end_offset" : 26,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "Shot",
      "start_offset" : 26,
      "end_offset" : 30,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "SD",
      "start_offset" : 31,
      "end_offset" : 33,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "500",
      "start_offset" : 33,
      "end_offset" : 36,
      "type" : "word",
      "position" : 7
    }
  ]
}
----
////

[analysis-word-delimiter-tokenfilter-analyzer-ex]]
==== Add to an analyzer

The following <<indices-create-index,create index API>> request uses the
`word_delimiter_graph` filter to configure a new
<<analysis-custom-analyzer,custom analyzer>>.

[source,console]
----
PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [ "word_delimiter_graph" ]
        }
      }
    }
  }
}
----

[WARNING]
====
Avoid using the `word_delimiter_graph` filter with tokenizers that remove
punctuation, such as the <<analysis-standard-tokenizer,`standard`>> tokenizer.
This could prevent the `word_delimiter_graph` filter from splitting tokens
correctly. It can also interfere with the filter's configurable parameters, such
as <<word-delimiter-graph-tokenfilter-catenate-all,`catenate_all`>> or
<<word-delimiter-graph-tokenfilter-preserve-original,`preserve_original`>>. We
recommend using the <<analysis-whitespace-tokenizer,`whitespace`>> tokenizer
instead.
====

[[word-delimiter-graph-tokenfilter-configure-parms]]
==== Configurable parameters

[[word-delimiter-graph-tokenfilter-adjust-offsets]]
`adjust_offsets`::
+
--
(Optional, boolean)
If `true`, the filter adjusts the offsets of split or catenated tokens to better
reflect their actual position in the token stream. Defaults to `true`.

[WARNING]
====
Set `adjust_offsets` to `false` if your analyzer uses filters, such as the
<<analysis-trim-tokenfilter,`trim`>> filter, that change the length of tokens
without changing their offsets. Otherwise, the `word_delimiter_graph` filter
could produce tokens with illegal offsets.
====
--

[[word-delimiter-graph-tokenfilter-catenate-all]]
`catenate_all`::
(Optional, boolean)
If `true`, the filter produces catenated tokens for chains of alphanumeric
characters separated by non-alphabetic delimiters. For example:
`wi-fi-232-enabled` -> [**`wifi232enabled`**, `wi`, `fi`, `232`, `enabled` ].
Defaults to `false`.

[[word-delimiter-graph-tokenfilter-catenate-numbers]]
`catenate_numbers`::
(Optional, boolean)
If `true`, the filter produces catenated tokens for chains of numeric characters
separated by non-alphabetic delimiters. For example: `01-02-03` ->
[**`010203`**, `01`, `02`, `03` ]. Defaults to `false`.

[[word-delimiter-graph-tokenfilter-catenate-words]]
`catenate_words`::
(Optional, boolean)
If `true`, the filter produces catenated tokens for chains of alphabetical
characters separated by non-alphabetic delimiters. For example: `wi-fi-enabled`
-> [**`wifienabled`**, `wi`, `fi`, `enabled`]. Defaults to `false`.

`generate_number_parts`::
(Optional, boolean)
If `true`, the filter includes tokens consisting of only numeric characters in
the output. If `false`, the filter excludes these tokens from the output.
Defaults to `true`.

`generate_word_parts`::
(Optional, boolean)
If `true`, the filter includes tokens consisting of only alphabetical characters
in the output. If `false`, the filter excludes these tokens from the output.
Defaults to `true`.

[[word-delimiter-graph-tokenfilter-preserve-original]]
`preserve_original`::
(Optional, boolean)
If `true`, the filter includes the original version of any split tokens in the
output. This original version includes non-alphanumeric delimiters. For example:
`wi-fi-232` -> [**`wi-fi-232`**, `wi`, `fi`, `232` ]. Defaults to `false`.

`protected_words`::
(Optional, array of strings)
Array of tokens the filter won't split.

`protected_words_path`::
+
--
(Optional, string)
Path to a file that contains a list of tokens the filter won't split.

This path must be absolute or relative to the `config` location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line
break.
--

`split_on_case_change`::
(Optional, boolean)
If `true`, the filter splits tokens at letter case transitions. For example:
`camelCase` -> [ `camel`, `Case`]. Defaults to `true`.

`split_on_numerics`::
(Optional, boolean)
If `true`, the filter splits tokens at letter-number transitions. For example:
`j2se` -> [ `j`, `2`, `se` ]. Defaults to `true`.

`stem_english_possessive`::
(Optional, boolean)
If `true`, the filter removes the English possessive (`'s`) from the end of each
token. For example: `O'Neil's` -> `[ `O`, `Neil` ]. Defaults to `true`.

`type_table`::
+
--
(Optional, array of strings)
Array of custom type mappings for characters. This allows you to map
non-alphanumeric characters as numeric or alphanumeric to avoid splitting on
those characters.

For example, the following array maps the plus (`+`) and hyphen (`-`) characters
as alphanumeric, which means they won't be treated as delimiters:

`["+ => ALPHA", "- => ALPHA"]`

Supported types include:

* `ALPHA` (Alphabetical)
* `ALPHANUM` (Alphanumeric)
* `DIGIT` (Numeric)
* `LOWER` (Lowercase alphabetical)
* `SUBWORD_DELIM` (Non-alphanumeric delimiter)
* `UPPER` (Uppercase alphabetical)
--

`type_table_path`::
+
--
(Optional, string)
Path to a file that contains custom type mappings for characters. This allows
you to map non-alphanumeric characters as numeric or alphanumeric to avoid
splitting on those characters.

For example, the contents of this file may contain the following:

[source,txt]
----
# Map the $, %, '.', and ',' characters to DIGIT
# This might be useful for financial data.
$ => DIGIT
% => DIGIT
. => DIGIT
\\u002C => DIGIT

# in some cases you might not want to split on ZWJ
# this also tests the case where we need a bigger byte[]
# see http://en.wikipedia.org/wiki/Zero-width_joiner
\\u200D => ALPHANUM
----

Supported types include:

* `ALPHA` (Alphabetical)
* `ALPHANUM` (Alphanumeric)
* `DIGIT` (Numeric)
* `LOWER` (Lowercase alphabetical)
* `SUBWORD_DELIM` (Non-alphanumeric delimiter)
* `UPPER` (Uppercase alphabetical)

This file path must be absolute or relative to the `config` location, and the
file must be UTF-8 encoded. Each mapping in the file must be separated by a line
break.
--

[[analysis-word-delimiter-graph-tokenfilter-customize]]
==== Customize

To customize the `word_delimiter_graph` filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.

For example, the following request creates a `word_delimiter_graph`
filter that uses the following rules:

* Split tokens at non-alphanumeric characters, _except_ the hyphen (`-`)
  character.
* Remove leading or trailing delimiters from each token.
* Do _not_ split tokens at letter case transitions.
* Do _not_ split tokens at letter-number transitions.
* Remove the English possessive (`'s`) from the end of each token.

[source,console]
----
PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "my_custom_word_delimiter_graph_filter" ]
        }
      },
      "filter": {
        "my_custom_word_delimiter_graph_filter": {
          "type": "word_delimiter_graph",
          "type_table": [ "- => ALPHA" ],
          "split_on_case_change": false,
          "split_on_numerics": false,
          "stem_english_possessive": true
        }
      }
    }
  }
}
----

[[analysis-word-delimiter-graph-differences]]
==== Differences between `word_delimiter` and `word_delimiter_graph`

Both the <<analysis-word-delimiter-tokenfilter,`word_delimiter`>> and
`word_delimiter_graph` token filters can produce catenated tokens when any of
the following parameters are `true`:

 * <<word-delimiter-graph-tokenfilter-catenate-all,`catenate_all`>>
 * <<word-delimiter-graph-tokenfilter-catenate-numbers,`catenate_numbers`>>
 * <<word-delimiter-graph-tokenfilter-catenate-words,`catenate_words`>>

When adding these new tokens to a stream, the `word_delimiter` filter places
catenated tokens _after_ the first delimited token. For example, with
`catenate_words` set to `true`, the `word_delimiter` filter changes [ `the`,
`wi-fi`, `is`, `enabled`]  to [`the`, `wi`, **`wifi`**, `fi`, `is`, `enabled` ].

This can cause issues for the <<query-dsl-match-query-phrase,`match_phrase`>>
query and other queries that rely on the sequence of token streams for matching.

The `word_delimiter_graph` filter places catenated tokens _before_ the first
delimited token. For example, with `catenate_words` set to `true`, the
`word_delimiter_graph` filter changes [ `the`, `wi-fi`, `is`, `enabled` ] to
[ `the`, **`wifi`**, `wi`, `fi`, `is`, `enabled` ].

This better preserves the token stream's original sequence and doesn't usually
interfere with `match_phrase` or similar queries.

The `word_delimiter_graph` also supports the
<<word-delimiter-graph-tokenfilter-adjust-offsets,`adjust_offsets`>> parameter,
which adjusts the offsets of split or catenated tokens to reflect their actual
position in the token stream. The `adjust_offsets` parameter is not supported by
the `word_delimiter` filter.