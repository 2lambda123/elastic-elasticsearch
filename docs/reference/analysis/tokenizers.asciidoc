[[analysis-tokenizers]]
== Tokenizers

A tokenizer splits the individual terms in a stream of text apart 
on boundaries such as whitespace and punctuation, and then outputs 
a set of tokens that can be indexed. A single tokenizer must be
configured for each <<analysis-analyzers, analyzer>>. 

Each ready-to-use analyzer wraps one of the built-in tokenizers. 
You can also use any of these tokenizers in a 
<<analysis-custom-analyzer,custom analyzer>>.

The <<analysis-standard-tokenizer, standard>> tokenizer is the recommended 
general use tokenizer and is the one used by the <<analysis-standard-analyzer, standard>>
analyzer. 

include::tokenizers/classic-tokenizer.asciidoc[]

include::tokenizers/edgengram-tokenizer.asciidoc[]

include::tokenizers/keyword-tokenizer.asciidoc[]

include::tokenizers/letter-tokenizer.asciidoc[]

include::tokenizers/lowercase-tokenizer.asciidoc[]

include::tokenizers/ngram-tokenizer.asciidoc[]

include::tokenizers/pathhierarchy-tokenizer.asciidoc[]

include::tokenizers/pattern-tokenizer.asciidoc[]

include::tokenizers/standard-tokenizer.asciidoc[]

include::tokenizers/thai-tokenizer.asciidoc[]

include::tokenizers/uaxurlemail-tokenizer.asciidoc[]

include::tokenizers/whitespace-tokenizer.asciidoc[]



