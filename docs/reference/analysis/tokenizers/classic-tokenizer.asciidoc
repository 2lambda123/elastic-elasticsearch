[[analysis-classic-tokenizer]]
=== Classic Tokenizer

The `classic` tokenizer is a grammar based tokenizer that generally splits words
at whitespace and punctuation characters, but treats email addresses, internet 
hostnames and hyphenated strings that contain a number as single tokens. It also
has heuristics for recognizing acronyms and company names. 

This tokenizer is generally a good choice for English language documents. However,
the tokenization rules don't always produce the desired results and it often
doesn't work well for languages other than English. The <<analysis-standard-tokenizer, `standard`>>
tokenizer is a better choice if you are working with other languages.

You can configure the following settings for a `classic` tokenizer:

[cols="<,<",options="header",]
|=======================================================================
|Setting |Description
|`max_token_length` |The maximum token length. If a token is seen that
exceeds this length then it is discarded. Defaults to `255`.
|=======================================================================

For example, using the `classic` tokenizer to process the string 
"Let the wild rumpus start! (bit.ly/1wsCiKv)" generates the following tokens:

 `Let, the, wild, rumpus, start, bit.ly, 1wsCiKv`

 [source,js]
--------------------------------------------------------------------------------
include::../../snippets/analysis/classic-tokenizer.json[]
--------------------------------------------------------------------------------
// SENSE: analysis/classic-tokenizer.json