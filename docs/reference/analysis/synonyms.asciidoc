[[synonyms]]
== Synonyms
++++
<titleabbrev>Synonyms</titleabbrev>
++++

Synonyms are words or phrases that have the same or similar meaning.
They are part of our everyday speech, and we are used to naturally use them and understand when we are referring to the same meaning using a different word.

Synonyms are an important aspect for searching.
They can improve the search experience and the relevancy of search results.

Synonyms will allow you to:

* Improve search relevance: Synonyms enable users to find relevant documents even if they use different terms to express the same concept. Documents that containing different terms but related meanings can be retrieved, making your search more flexible and inclusive.
* Increase search scope: Using synonyms ensures a wider range of possible terms. Search results are not limited to the specific terms included in your documents, and variations of relevant terms are included in search results.
* Domain-specific vocabulary: Certain domains use specific terms that users are not familiar with. Using synonyms allow search to become more user-friendly, as users can search terms they are familiar with and search results will not be limited to the specific domain vocabulary.
* Handling common misspellings and typos: Synonyms can be used to make sure that frequent or specific misspellings are correctly handled.

[discrete]
[[synonyms-in-elasticsearch]]
=== Synonyms in {es}

{es} uses synonyms as part of the <<analysis-overview,analysis process>>.
Analyzers can be applied at <<analysis-index-search-time,index time or search time>>.


==== Synonym token filters

There are two different <<analysis-tokenfilters,token filters>> that can be used for including synonyms:

* <<analysis-synonym-tokenfilter>>: Use it for single word synonyms ("car", "automobile").
* <<analysis-synonym-graph-tokenfilter>>: Use it for synonyms that may span multiple words ("hurriedly", "in a hurry").


==== Define your synonyms

Synonyms are defined using *synonym rules*.
Each synonym rule contains words that are synonyms.

Synonyms are grouped together using *synonyms sets*.
You can have as many synonyms sets as you need.

You can use two formats to define synonyms: Solr and WordNet.

===== Solr format

This format uses two different definitions:

* Equivalent synonyms: Define groups of words that are equivalent. Words are separated by commas.

Example:
    * ipod, i-pod, i pod
    * computer, pc, laptop

* Explicit mappings: Matches any of the words defined to other words. Words on the left hand side of the rule definition are expanded into all the possibilities described on the right hand side.

Example:
    * personal computer => pc
    * sea biscuit, sea biscit => seabiscuit


===== WordNet format

https://wordnet.princeton.edu/[WordNet] defines synonyms sets spanning multiple lines. Each line contains the following information:
* Synonyms set numeric identifier
* Ordinal of the synonym in the synonyms set
* Synonym word
* Word type identifier: Noun (n), verb (v), adjective (a) or adverb (b).
* Depth of the word in the synonym net

The following example defines a synonym set for the words "come", "advance" and "approach":

``s(100000002,1,'come',v,1,0).
s(100000002,2,'advance',v,1,0).
s(100000002,3,'approach',v,1,0).""";
``
Synonyms sets are applied to your <<mappings,field mappings>> via analyzers.


==== Store your synonyms

Your synonyms sets need to be stored in {es} so your analyzers can refer to them.
There are three ways to include your synonyms sets in {es}:

[[synonyms-synonyms-api]]
===== Synonyms API
You can use <<synonyms-apis,synonyms APIs>> to manage synonyms sets.
This is the most flexible approach, as it allows to dynamically define and modify synonyms sets.

Synonyms sets created via synonyms APIs can be included into synonyms token filters using `synonyms_set` configuration:

```
  "filter": {
    "synonyms_filter": {
      "type": "synonym_graph",
      "synonyms_set": "my-synonym-set"
    }
  }
```

Changes in your synonyms sets will automatically reload the associated analyzers.

===== Synonyms File
You can store your synonyms set in a file.

A synonyms set file needs to be uploaded to all your cluster nodes, and be located in the configuration directory for your {es} distribution.
If you're using {ess}, you can upload synonyms fies using {cloud}/ec-custom-bundles.html[custom bundles].

The path to the file needs to be included as part of your token filter using `synonyms_path` configuration:

```
  "filter": {
    "synonyms_filter": {
      "type": "synonym_graph",
      "synonyms_path": "analysis/synonym-set.txt"
    }
  }
```

Keep in mind that using files implies that your synonyms sets fies need to be uploaded and kept in sync on all nodes in your cluster.
This is a less flexible approach than using the <<synonyms-synonyms-api,synonyms API>>.

===== Inline
For testing out your synonyms, you can directly inline your synonyms in your token filter definition.

Include your synonyms in your token filter using the `synonyms` configuration:

```
  "filter": {
    "synonyms_filter": {
      "type": "synonym_graph",
      "synonyms": ["pc => personal computer", "computer, pc, laptop"]
    }
  }
```

This is not recommended for production usage, as a large number of synonyms impacts index mapping size and can lead to performance issues.

==== Index or Search time

You need to decide when to apply your synonyms:

* Index time: Synonyms are applied when the documents are indexed into Elasticsearch. This means that your synonyms are stored as part of your documents. It is a less flexible alternative, as changing your synonyms imply that your index needs to be <<docs-reindex,reindexed>> to ensure the new synonyms are applied and stored in your index.
* Search time: Synonyms are applied when a search is executed. This is a more flexible approach, as indices don't have to be reindexed, and search analyzers <<indices-reload-analyzers,can be reloaded>> with changes done to synonyms.

[discrete]
[[tokenization]]
=== Tokenization

Analysis makes full-text search possible through _tokenization_: breaking a text
down into smaller chunks, called _tokens_. In most cases, these tokens are
individual words.

If you index the phrase `the quick brown fox jumps` as a single string and the
user searches for `quick fox`, it isn't considered a match. However, if you
tokenize the phrase and index each word separately, the terms in the query
string can be looked up individually. This means they can be matched by searches
for `quick fox`, `fox brown`, or other variations.

[discrete]
[[normalization]]
=== Normalization

Tokenization enables matching on individual terms, but each token is still
matched literally. This means:

*  A search for `Quick` would not match `quick`, even though you likely want
either term to match the other

* Although `fox` and `foxes` share the same root word, a search for `foxes`
would not match `fox` or vice versa.

* A search for `jumps` would not match `leaps`. While they don't share a root
word, they are synonyms and have a similar meaning.

To solve these problems, text analysis can _normalize_ these tokens into a
standard format. This allows you to match tokens that are not exactly the same
as the search terms, but similar enough to still be relevant. For example:

* `Quick` can be lowercased: `quick`.

* `foxes` can be _stemmed_, or reduced to its root word: `fox`.

* `jump` and `leap` are synonyms and can be indexed as a single word: `jump`.

To ensure search terms match these words as intended, you can apply the same
tokenization and normalization rules to the query string. For example, a search
for `Foxes leap` can be normalized to a search for `fox jump`.

[discrete]
[[analysis-customization]]
=== Customize text analysis

Text analysis is performed by an <<analyzer-anatomy,_analyzer_>>, a set of rules
that govern the entire process.

{es} includes a default analyzer, called the
<<analysis-standard-analyzer,standard analyzer>>, which works well for most use
cases right out of the box.

If you want to tailor your search experience, you can choose a different
<<analysis-analyzers,built-in analyzer>> or even
<<analysis-custom-analyzer,configure a custom one>>. A custom analyzer gives you
control over each step of the analysis process, including:

* Changes to the text _before_ tokenization

* How text is converted to tokens

* Normalization changes made to tokens before indexing or search
