[[analysis]]
= Text Analysis

[partintro]
--
An Elasticsearch index maps _terms_ to the documents in which they occur. To
improve search results, most text fields are analyzed to
determine what terms should be stored in the index. Similarly, at search time,
the search query is analyzed to determine what terms to look up in the index.

This text analysis process is performed by an _analyzer_. An analyzer breaks the
text in a field down into individual terms and normalizes the terms so they can
be stored in a standard form that facilitates matching and increases
search result accuracy. For example, terms might be converted to lowercase and
filtered to remove commonly-occuring or insignificant terms. Elasticsearch
analyzers map to the
https://lucene.apache.org/core/4_3_0/core/org/apache/lucene/analysis/Analyzer.html[Analyzer] class in Lucene.

Elasticsearch provides several ready-to-use analyzers. By default, the
<<analysis-standard-analyzer, `standard`>> analyzer is used to split text
on word boundaries, convert the tokens to lowercase, and remove stopwords. While
the `standard` analyzer is a good choice for most Western languages, if you are
indexing content that's in a particular language or need more control over the
analysis process, you can use a different analyzer or configure a custom analyzer.

You can configure analyzers on a per-index basis in the index settings.
You control which analyzer is used to process each text field by specifying an
analyzer in the field <<mapping, mapping>>. Similarly, you can specify which
analyzer you want to use when you submit a <<search, search>> query. Normally, you want to
use the same analyzer for both indexing and searching.

For more information about configuring and using analyzers, see <<analysis-analyzers, Analyzers>>.
--

include::analysis/analyzers.asciidoc[]

include::analysis/tokenizers.asciidoc[]

include::analysis/tokenfilters.asciidoc[]

include::analysis/charfilters.asciidoc[]

include::analysis/icu-plugin.asciidoc[]