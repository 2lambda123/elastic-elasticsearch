[role="xpack"]
[testenv="platinum"]
[[evaluate-dfanalytics]]
=== Evaluate {dfanalytics} API

[subs="attributes"]
++++
<titleabbrev>Evaluate {dfanalytics}</titleabbrev>
++++

Evaluates the {dfanalytics} for an annotated index.

experimental[]

[[ml-evaluate-dfanalytics-request]]
==== {api-request-title}

`POST _ml/data_frame/_evaluate`


[[ml-evaluate-dfanalytics-prereq]]
==== {api-prereq-title}

* You must have `monitor_ml` privilege to use this API. For more 
information, see <<security-privileges>> and <<built-in-roles>>.


[[ml-evaluate-dfanalytics-desc]]
==== {api-description-title}

The API packages together commonly used evaluation metrics for various types of 
machine learning features. This has been designed for use on indexes created by 
{dfanalytics}. Evaluation requires both a ground truth field and an analytics 
result field to be present.


[[ml-evaluate-dfanalytics-request-body]]
==== {api-request-body-title}

`index`::
  (Required, object) Defines the `index` in which the evaluation will be
  performed.

`query`::
  (Optional, object) A query clause that retrieves a subset of data from the 
  source index. See <<query-dsl>>.

`evaluation`::
  (Required, object) Defines the type of evaluation you want to perform. See 
  <<ml-evaluate-dfanalytics-resources>>.
+
--
Available evaluation types:

* `binary_soft_classification`
* `regression`
* `classification`
--


////
[[ml-evaluate-dfanalytics-results]]
==== {api-response-body-title}

`binary_soft_classification`::
  (object) If you chose to do binary soft classification, the API returns the
  following evaluation metrics:
  
`auc_roc`::: TBD

`confusion_matrix`::: TBD
  
`precision`::: TBD

`recall`::: TBD
////

[[ml-evaluate-dfanalytics-example]]
==== {api-examples-title}

===== Binary soft classification

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
  "index": "my_analytics_dest_index",
  "evaluation": {
    "binary_soft_classification": {
      "actual_field": "is_outlier",
      "predicted_probability_field": "ml.outlier_score"
    }
  }
}
--------------------------------------------------
// TEST[skip:TBD]

The API returns the following results:

[source,console-result]
----
{
  "binary_soft_classification": {
    "auc_roc": {
      "score": 0.92584757746414444
    },
    "confusion_matrix": {
      "0.25": {
          "tp": 5,
          "fp": 9,
          "tn": 204,
          "fn": 5
      },
      "0.5": {
          "tp": 1,
          "fp": 5,
          "tn": 208,
          "fn": 9
      },
      "0.75": {
          "tp": 0,
          "fp": 4,
          "tn": 209,
          "fn": 10
      }
    },
    "precision": {
        "0.25": 0.35714285714285715,
        "0.5": 0.16666666666666666,
        "0.75": 0
    },
    "recall": {
        "0.25": 0.5,
        "0.5": 0.1,
        "0.75": 0
    }
  }
}
----


===== {regression-cap}

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
  "index": "house_price_predictions", <1>
  "query": {
      "bool": {
        "filter": [
          { "term":  { "ml.is_training": false } } <2>
        ]
      }
  },
  "evaluation": {
    "regression": { 
      "actual_field": "price", <3>
      "predicted_field": "ml.price_prediction", <4>
      "metrics": {  
        "r_squared": {},
        "mean_squared_error": {}                             
      }
    }
  }
}
--------------------------------------------------
// TEST[skip:TBD]

<1> The output destination index from a {dfanalytics} {reganalysis}.
<2> In this example, a test/train split (`training_percent`) was defined for the 
{reganalysis}. This query limits evaluation to be performed on the test split 
only. 
<3> The ground truth value for the actual house price. This is required in order 
to evaluate results.
<4> The predicted value for house price calculated by the {reganalysis}.


The following example calculates the training error:

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
  "index": "student_performance_mathematics_reg",
  "query": {
    "term": {
      "ml.is_training": {
        "value": true <1>
      }
    }
  },
  "evaluation": {
    "regression": { 
      "actual_field": "G3", <2>
      "predicted_field": "ml.G3_prediction", <3>
      "metrics": {  
        "r_squared": {},
        "mean_squared_error": {}                             
      }
    }
  }
}
--------------------------------------------------
// TEST[skip:TBD]

<1> In this example, a test/train split (`training_percent`) was defined for the 
{reganalysis}. This query limits evaluation to be performed on the train split 
only. It means that a training error will be calculated.
<2> The field that contains the ground truth value for the actual student 
performance. This is required in order to evaluate results.
<3> The field that contains the predicted value for student performance 
calculated by the {reganalysis}.


The next example calculates the testing error. The only difference compared with 
the previous example is that `ml.is_training` is set to `false` this time, so 
the query excludes the train split from the evaluation.

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
  "index": "student_performance_mathematics_reg",
  "query": {
    "term": {
      "ml.is_training": {
        "value": false <1>
      }
    }
  },
  "evaluation": {
    "regression": { 
      "actual_field": "G3", <2>
      "predicted_field": "ml.G3_prediction", <3>
      "metrics": {  
        "r_squared": {},
        "mean_squared_error": {}                             
      }
    }
  }
}
--------------------------------------------------
// TEST[skip:TBD]

<1> In this example, a test/train split (`training_percent`) was defined for the 
{reganalysis}. This query limits evaluation to be performed on the test split 
only. It means that a testing error will be calculated.
<2> The field that contains the ground truth value for the actual student 
performance. This is required in order to evaluate results.
<3> The field that contains the predicted value for student performance 
calculated by the {reganalysis}.


===== {classification-cap}

In the first example, `size` is not provided. The full matrix returns in the 
response as the actual size is less than the maximum size (1000). 


[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{ 
   "index": "animal_classification",
   "evaluation": {
      "classification": { <1>
         "actual_field": "animal_class", <2>
         "predicted_field": "animal_class_prediction" <3>
      }
   }
}
--------------------------------------------------
// TEST[skip:TBD]

<1> The evaluation type.
<2> The field that contains the ground truth value for the actual animal 
classification. This is required in order to evaluate results.
<3> The field that contains the predicted value for animal classification by 
the {classanalysis}.


The API returns the following result:

[source,console-result]
--------------------------------------------------
{
   "classification" : {
      "multiclass_confusion_matrix" : {
         "confusion_matrix" : {
            "ant" : {
               "mouse" : 2,
               "horse" : 1,
               "dog" : 3,
               "ant" : 5,
               "cat" : 4
            },
            "cat" : {
               "ant" : 4,
               "cat" : 5,
               "dog" : 3,
               "horse" : 1,
               "mouse" : 2
            },
            "dog" : {
               "dog" : 5,
               "cat" : 4,
               "ant" : 3,
               "mouse" : 2,
               "horse" : 1
            },
            "horse" : {
               "mouse" : 2,
               "horse" : 5,
               "dog" : 3,
               "ant" : 1,
               "cat" : 4
            },
            "mouse" : {
               "mouse" : 5,
               "horse" : 1,
               "dog" : 3,
               "ant" : 2,
               "cat" : 4
            }
         }
      }
   }
}
--------------------------------------------------


In the second example the `size` is still not provided, but this time the 
response contains only the restricted matrix as the actual size is grater than 
the maximum size (1000).

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{ 
   "index": "my-index",
   "evaluation": { 
      "classification": { 
         "actual_field": "a",
         "predicted_field": "b"
      }
   }
}
--------------------------------------------------
// TEST[skip:TBD]


The API returns the following result:

[source,console-result]
--------------------------------------------------
{
   "classification" : {
      "multiclass_confusion_matrix" : {
         "confusion_matrix" : {
            "class_1" : {
               "class_1" : 5, <1>
               ...
               "class_100" : 4, <2>
               "_other_": 6 <3>
            },
            "class_2" : {
               "class_1" : 5,
               ...
               "class_100" : 3,
               "_other_": 7
            },
            ...
            "class_100" : {
               "class_1" : 15,
               ...
               "class_100" : 30,
            }
         },
         "_other_" : 25 <4>
      }
   }
}
--------------------------------------------------
<1> Number of datapoints that are classified correctly as class_1.
<2> Number of datapoints that are misclassified as class_100.
<3> Number of datapoints that are misclassified as one of the classes that 
doesn't fit into the response specifically.
<4> Number of actual classes that doesn't fit into the response specifically.


In the third example, the `size` is provided, the response only contains the 
restricted matrix as the actual size is greater than the maximum size (1000).

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{ 
   "index": "my-index",
   "evaluation": { 
      "classification": { 
         "actual_field": "a",
         "predicted_field": "b",
         "metrics": {
           "multiclass_confusion_matrix": {
             "size": 3
           }
         }
      }
   }
}
--------------------------------------------------


The API returns the following results:

[source,console-result]
--------------------------------------------------
{
   "classification" : {
      "multiclass_confusion_matrix" : {
         "confusion_matrix" : {
            "ant" : {
               "ant" : 5,
               "cat" : 4,
               "dog" : 0,
               "_other_": 6
            },
            "cat" : {
               "ant" : 0,
               "cat" : 5,
               "dog" : 3,
               "_other_": 7
            },
            "dog" : {
               "ant" : 0,
               "cat" : 0,
               "dog" : 15
            }
         },
         "_other_" : 2
      }
   }
}
--------------------------------------------------
