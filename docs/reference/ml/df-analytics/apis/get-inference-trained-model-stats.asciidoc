[role="xpack"]
[testenv="basic"]
[[get-inference-stats]]
=== Get {infer} trained model statistics API
[subs="attributes"]
++++
<titleabbrev>Get {infer} trained model stats</titleabbrev>
++++

Retrieves usage information for trained {infer} models.

experimental[]


[[ml-get-inference-stats-request]]
==== {api-request-title}

`GET _ml/inference/_stats` +

`GET _ml/inference/_all/_stats` +

`GET _ml/inference/<model_id>/_stats` +

`GET _ml/inference/<model_id>,<model_id_2>/_stats` +

`GET _ml/inference/<model_id_pattern*>,<model_id_2>/_stats`


[[ml-get-inference-stats-prereq]]
==== {api-prereq-title}

If the {es} {security-features} are enabled, you must have the following 
privileges:

* cluster: `monitor_ml`
  
For more information, see <<security-privileges>> and <<built-in-roles>>.


[[ml-get-inference-stats-desc]]
==== {api-description-title}

You can get usage information for multiple trained models in a single API 
request by using a comma-separated list of model IDs or a wildcard expression.


[[ml-get-inference-stats-path-params]]
==== {api-path-parms-title}

`<model_id>`::
(Optional, string) 
include::{es-repo-dir}/ml/ml-shared.asciidoc[tag=model-id]


[[ml-get-inference-stats-query-params]]
==== {api-query-parms-title}

`allow_no_match`::
(Optional, boolean) 
include::{es-repo-dir}/ml/ml-shared.asciidoc[tag=allow-no-match]

`from`::
(Optional, integer) 
include::{es-repo-dir}/ml/ml-shared.asciidoc[tag=from]

`size`::
(Optional, integer) 
include::{es-repo-dir}/ml/ml-shared.asciidoc[tag=size]


[[ml-get-inference-stats-response-codes]]
==== {api-response-codes-title}

`404` (Missing resources)::
  If `allow_no_match` is `false`, this code indicates that there are no
  resources that match the request or only partial matches for the request.

[role="child_attributes"]
[[ml-get-inference-stats-results]]
==== {api-response-body-title}

`count`::
(integer)
The total number of trained model statistics that matched the requested ID patterns.
Could be higher than the number of items in the `trained_model_stats` array.

`trained_model_stats`::
(array)
An array of trained model statistics, which are sorted by the `model_id` value in
ascending order.
+
.Properties of trained model stats
[%collapsible%open]
====
`model_id`:::
(string)
include::{es-repo-dir}/ml/ml-shared.asciidoc[tag=model-id]

`pipeline_count`:::
(integer)
How many ingest pipelines currently refer to the model.

`inference_stats`:::
(object)
A collection of inference stats fields.
+
.Properties of inference stats
[%collapsible%open]
=====

`missing_all_fields_count`:::
(integer)
How many inference calls were missing all their features

`inference_count`:::
(integer)
How many times inference were called

`cache_miss_count`:::
(integer)
How many times the model was loaded for inference and NOT from
cache.

`failure_count`:::
(integer)
The count of failures when the using the model for inference

`timestamp`:::
(<<time-units,time units>>)
When these stats were last gathered.
=====

`ingest`:::
(object)
A collection of ingest stats for the model. See the `ingest` section in
<<cluster-nodes-stats,Nodes Stats>> for definition.

====

[[ml-get-inference-stats-example]]
==== {api-examples-title}

The following example gets usage information for all the trained models:

[source,console]
--------------------------------------------------
GET _ml/inference/_stats
--------------------------------------------------
// TEST[skip:TBD]


The API returns the following results:

[source,console-result]
----
{
  "count": 2,
  "trained_model_stats": [
    {
      "model_id": "flight-delay-prediction-1574775339910",
      "pipeline_count": 0,
      "inference_stats": {
        "failure_count": 0,
        "inference_count": 4,
        "cache_miss_count": 3,
        "missing_all_fields_count": 0,
        "timestamp": 1592399986979
      }
    },
    {
      "model_id": "regression-job-one-1574775307356",
      "pipeline_count": 1,
      "inference_stats": {
        "failure_count": 0,
        "inference_count": 178,
        "cache_miss_count": 3,
        "missing_all_fields_count": 0,
        "timestamp": 1592399986979
      },
      "ingest": {
        "total": {
          "count": 178,
          "time_in_millis": 8,
          "current": 0,
          "failed": 0
        },
        "pipelines": {
          "flight-delay": {
            "count": 178,
            "time_in_millis": 8,
            "current": 0,
            "failed": 0,
            "processors": [
              {
                "inference": {
                  "type": "inference",
                  "stats": {
                    "count": 178,
                    "time_in_millis": 7,
                    "current": 0,
                    "failed": 0
                  }
                }
              }
            ]
          }
        }
      }
    }
  ]
}
----
// NOTCONSOLE
