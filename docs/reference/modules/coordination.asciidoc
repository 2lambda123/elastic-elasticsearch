[[modules-cluster-coordination]]
== Cluster coordination

The cluster coordination module is responsible for electing a master node and
managing changes to the cluster state.

[float]
=== Quorum-based decision making

Electing a master node and changing the cluster state are the two fundamental
tasks that master-eligible nodes must work together to perform. It is important
that these activities work robustly even if some nodes have failed, and
Elasticsearch achieves this robustness by only considering each action to have
succeeded on receipt of responses from a _quorum_, a subset of the
master-eligible nodes in the cluster. The advantage of requiring only a subset
of the nodes to respond is that it allows for some of the nodes to fail without
preventing the cluster from making progress, and the quorums are carefully
chosen so as not to allow the cluster to "split brain", i.e. to be partitioned
into two pieces each of which may make decisions that are inconsistent with
those of the other piece.

Elasticsearch allows you to add and remove master-eligible nodes to a running
cluster. In many cases you can do this simply by starting or stopping the nodes
as required, as described in more detail below. As nodes are added or removed
Elasticsearch maintains an optimal level of fault tolerance by updating the
cluster's _voting configuration_, which is the set of master-eligible nodes
whose responses are counted when making decisions such as electing a new master
or committing a new cluster state. A decision is only made once more than half
of the nodes in the voting configuration have responded. Usually the voting
configuration is the same as the set of all the master-eligible nodes that are
currently in the cluster, but there are some situations in which they may be
different. As long as more than half of the nodes in the voting configuration
are still healthy then the cluster can still make progress.

[float]
=== Cluster maintenance, rolling restarts and migrations

Many cluster maintenance tasks involve temporarily shutting down one or more
nodes and then starting them back up again. By default Elasticsearch can remain
available if one of its master-eligible nodes is taken offline, such as during
a <<rolling-upgrades,rolling restart>>, and if multiple nodes are stopped and
then started again then it will automatically recover, such as during a
<<restart-upgrade,full cluster restart>>. There is no need to take any further
action with the APIs described here in these cases, because the set of master
nodes is not changing permanently.

It is also possible to perform a migration of a cluster onto entirely new nodes
without taking the cluster offline, via a _rolling migration_. A rolling
migration is similar to a rolling restart, in that it is performed one node at
a time, and also requires no special handling for the master-eligible nodes as
long as there are at least two of them at all times.

TODO the above is only true if the maintenance happens slowly enough, otherwise
the configuration might not catch up. Need to add this to the rolling restart
docs.

[float]
==== Auto-reconfiguration

Nodes may join or leave the cluster, and Elasticsearch reacts by making
corresponding changes to the voting configuration in order to ensure that the
cluster is as resilient as possible. The default auto-reconfiguration behaviour
is expected to give the best results for almost all use-cases. The current
voting configuration is stored in the cluster state so you can inspect its
current contents as follows:

[source,js]
--------------------------------------------------
GET /_cluster/state?filter_path=TODO
--------------------------------------------------
// CONSOLE

Larger voting configurations are usually more resilient, so Elasticsearch will
normally prefer to add nodes to the voting configuration once they have joined
the cluster. Similarly, if a node in the voting configuration leaves the
cluster and there is another node in the cluster that is not in the voting
configuration then it is preferable to swap these two nodes over, leaving the
size of the voting configuration unchanged but increasing its resilience.

It is not so straightforward to automatically remove nodes from the voting
configuration after they have left the cluster, and different strategies have
different benefits and drawbacks, so the right choice depends on how the
cluster will be used and is controlled by the following setting.

`cluster.automatically_shrink_voting_configuration`::

    Defaults to `true`, meaning that the voting configuration will
    automatically shrink, shedding departed nodes, as long as it still contains
    at least 3 nodes.  If set to `false`, the voting configuration never
    automatically shrinks; departed nodes must be removed manually using the
    retirement API described below.

NOTE: If `cluster.automatically_shrink_voting_configuration` is set to `true`,
the recommended and default setting, and there are at least three
master-eligible nodes in the cluster, then Elasticsearch remains capable of
processing cluster-state updates as long as all but one of its master-eligible
nodes are healthy.

There are situations in which Elasticsearch might tolerate the loss of multiple
nodes, but this is not guaranteed under all sequences of failures. If this
setting is set to `false` then departed nodes must be removed from the voting
configuration manually, using the retirement API described below, to achieve
the desired level of resilience.

Note that Elasticsearch will not suffer from a "split-brain" inconsistency
however it is configured. This setting only affects its availability in the
event of the failure of some of its nodes.

[float]
==== Even numbers of master-eligible nodes

There should normally be an odd number of master-eligible nodes in a cluster.
If there is an even number then Elasticsearch will leave one of them out of the
voting configuration to ensure that it has an odd size. This does not decrease
the failure-tolerance of the cluster, and in fact improves it slightly: if the
cluster is partitioned into two even halves then one of the halves will contain
a majority of the voting configuration and will be able to keep operating,
whereas if all of the master-eligible nodes' votes were counted then neither
side could make any progress in this situation.

[float]
==== Adding master-eligible nodes

It is recommended to have a small, fixed, number of master-eligible nodes in a
cluster, and to scale the cluster up and down by adding and removing
non-master-eligible nodes only. However there are situations in which it may be
desirable to add extra master-eligible nodes to a cluster.

If you wish to add some master-eligible nodes to your cluster, simply configure
the new nodes to find the existing cluster and start them up. Elasticsearch
will add the new nodes to the voting configuration if it is appropriate to do
so.

[float]
==== Removing master-eligible nodes

It is recommended to have a small, fixed, number of master-eligible nodes in a
cluster, and to scale the cluster up and down by adding and removing
non-master-eligible nodes only. However there are situations in which it may be
desirable to remove some master-eligible nodes from a cluster.

It is important not to remove too many master-eligible nodes all at the same
time. For instance, if there are currently seven master-eligible nodes and you
wish to reduce this to three, it is not possible simply to stop four of the
nodes all at the same time: to do so would leave only three nodes remaining,
which is less than half of the voting configuration, which means the cluster
cannot take any further actions.

As long as there are at least three master-eligible nodes in the cluster, as a
general rule it is best to remove nodes one-at-a-time, allowing enough time for
the auto-reconfiguration to take effect after each removal.

If there are only two master-eligible nodes then neither node can be safely
removed since both are required to reliably make progress, so you must first
explicitly _retire_ one of the nodes. A retired node still works normally, but
Elasticsearch will try and remove it from the voting configuration so its vote
is no longer required, and will never move a retired node back into the voting
configuration after it has been removed. Once a node has been successfully
retired, it is safe to shut it down. A node can be retired using the following
API:

[source,js]
--------------------------------------------------
# Retire node and wait for its removal up to the default timeout of 30 seconds
POST /_nodes/node_name/retire
# Retire node and wait for its removal up to one minute
POST /_nodes/node_name/retire?timeout=1m
--------------------------------------------------
// CONSOLE

The node to retire is specified using <<cluster-nodes,node filters>> in place
of `node_name` here. If a call to the retirement API fails then the call can
safely be retried. A successful response guarantees that the node has been
removed from the voting configuration and will not be reinstated.

Although the retirement API is most useful for removing a node from a two-node
cluster, it is also possible to use it to remove nodes from larger clusters. If
removing multiple nodes from a cluster it is important not to remove too many
voting nodes too quickly, so that the voting configuration can be updated
between each removal, and this can be achieved by retiring the nodes too to
obtain confirmation that they are no longer in the voting configuration.  In
the example described above, shrinking a seven-master-node cluster down to only
have three master nodes, you could retire four of the nodes and then shut them
down simultaneously. 

A node (or nodes) can be brought back out of retirement using the `unretire`
API:

[source,js]
--------------------------------------------------
POST /_nodes/node_name/unretire
--------------------------------------------------
// CONSOLE

The node (or nodes) to reinstate are specified using <<cluster-nodes,node
filters>> in place of `node_name` here. After being brought back out of
retirement they might or might not immediately be added to the voting
configuration.

The current set of retired nodes is stored in the cluster state and can be
inspected as follows:

[source,js]
--------------------------------------------------
GET /_cluster/state?filter_path=TODO
--------------------------------------------------
// CONSOLE

This set is limited in size by the following setting:

`cluster.max_retired_nodes`::

    Sets a limits on the number of retired nodes at any one time. Defaults to
    `10`.

Because there can only be a limited number of retired nodes at once, once a
retired node has been destroyed its entry should be removed from the set of
retired nodes using the unretire API.

[float]
=== Cluster bootstrapping

There is a risk when starting up a brand-new cluster is that you accidentally
form two separate clusters instead of one. This could lead to data loss: you
might start using both clusters before noticing that anything had gone wrong,
and it will then be impossible to merge them together later.

NOTE: To illustrate how this could happen, imagine starting up a three-node
cluster in which each node knows that it is going to be part of a three-node
cluster. A majority of three nodes is two, so normally the first two nodes to
discover each other will form a cluster and the third node will join them a
short time later. However, imagine that four nodes were accidentally started
instead of three: in this case there are enough nodes to form two separate
clusters. Of course if each node is started manually then it's unlikely that
too many nodes are started, but it's certainly possible to get into this
situation if using a more automated orchestrator, particularly if a network
partition happens at the wrong time.

We avoid this by requiring a separate _cluster bootstrapping_ process to take
place on every brand-new cluster. This is only required the very first time the
whole cluster starts up: new nodes joining an established cluster can safely
obtain all the information they need from the elected master, and nodes that
have previously been part of a cluster will have stored to disk all the
information required when restarting.

A cluster can be bootstrapped by sending a _bootstrap warrant_ to any of its
master-eligible nodes.  A bootstrap warrant is a document that contains the
information that the cluster needs to finish forming, including the identities
of the master-eligible nodes that form its first voting configuration, and
looks like this:

[source,js]
--------------------------------------------------
{
  "master_nodes":[
    {"id":"USpTGYaBSIKbgSUJR2Z9lg","name":"master-a"},
    {"id":"gSUJR2Z9lgUSpTGYaBSIKb","name":"master-b"},
    {"id":"2Z9lgUSpTgSUYaBSIKbJRG","name":"master-c"}
  ]
}
--------------------------------------------------

To bootstrap a cluster, the administrator must identify a suitable set of
master-eligible nodes, construct a bootstrap warrant, and pass the warrant to
the `POST /_cluster/bootstrap` API:

[source,js]
--------------------------------------------------
# send the bootstrap warrant back to the cluster
POST /_cluster/bootstrap
{
  "master_nodes":[
    {"id":"USpTGYaBSIKbgSUJR2Z9lg","name":"master-a"},
    {"id":"gSUJR2Z9lgUSpTGYaBSIKb","name":"master-b"},
    {"id":"2Z9lgUSpTgSUYaBSIKbJRG","name":"master-c"}
  ]
}
--------------------------------------------------
// CONSOLE

This only needs to occur once, on a single master-eligible node in the cluster,
but for robustness it is safe to repeatedly call `POST /_cluster/bootstrap`,
and to call it on different nodes concurrently. However **it is vitally
important** to use the same bootstrap warrant in each call.

WARNING: You must pass the same bootstrap warrant to each call to `POST
/_cluster/bootstrap` in order to be sure that only a single cluster forms
during bootstrapping and therefore to avoid the risk of data loss.

The simplest and safest way to construct a bootstrap warrant is to use the `GET
/_cluster/bootstrap` API:

[source,js]
--------------------------------------------------
# Immediately return a bootstrap warrant based on the nodes discovered so far
GET /_cluster/bootstrap
# Wait until the node has discovered at least 3 nodes, or 60 seconds has elapsed,
# and then return the resulting bootstrap warrant
GET /_cluster/bootstrap?wait_for_nodes=3&timeout=60s
--------------------------------------------------
// CONSOLE

This API returns a properly-constructed bootstrap warrant that is ready to pass
to the `POST /_cluster/bootstrap` API.  It includes all of the master-eligible
nodes that the handling node has discovered via the gossip-based discovery
protocol, and returns an error if fewer nodes have been discovered than
expected.

It is also possible to construct a bootstrap warrant manually and to specify
the initial set of nodes in terms of their names alone, rather than including
their IDs too:

[source,js]
--------------------------------------------------
# send the bootstrap warrant back to the cluster
POST /_cluster/bootstrap
{
  "master_nodes":[
    {"name":"master-a"},
    {"name":"master-b"},
    {"name":"master-c"}
  ]
}
--------------------------------------------------
// CONSOLE

It is safer to include the node IDs, in case two nodes are accidentally started
with the same name.

This process is implemented in the `elasticsearch-bootstrap-cluster`
command-line tool:

[source,txt]
--------------------------------------------------
$ bin/elasticsearch-bootstrap-cluster --node http://10.0.12.1:9200/ \
    --node http://10.0.13.1:9200/ --node https://10.0.14.1:9200/
--------------------------------------------------

The arguments to this tool are the addresses of (some, preferably all, of) its
master-eligible nodes. The tool will construct a bootstrap warrant and then
bootstrap the cluster, retrying safely if any step fails.

[float]
=== Unsafe disaster recovery

In a disaster situation a cluster may have lost half or more of its
master-eligible nodes and therefore be in a state in which it cannot elect a
master. There is no way to recover from this situation without risking data
loss, but if there is no other viable path forwards then this may be necessary.
This can be performed with the following command on a surviving node:

[source,js]
--------------------------------------------------
POST /_cluster/force_local_node_takeover
--------------------------------------------------
// CONSOLE

This works by forcibly overriding the current voting configuration with one in
which the handling node is the only voting master, so that it forms a quorum on
its own. Because there is a risk of data loss when performing this command it
requires the `accept_data_loss` parameter to be set to `true` in the URL.
Afterwards, once the cluster has successfully formed,
`cluster.master_nodes_failure_tolerance` should be increased to a suitable
value.

[float]
=== Election scheduling

Elasticsearch uses an election process to agree on an elected master node, both
at startup and if the existing elected master fails. Any master-eligible node
can start an election, and normally the first election that takes place will
succeed. Elections only usually fail when two nodes both happen to start their
elections at about the same time, so elections are scheduled randomly on each
node to avoid this happening. Nodes will retry elections until a master is
elected, backing off on failure, so that eventually an election will succeed
(with arbitrarily high probability). The following settings control the
scheduling of elections.

`cluster.election.initial_timeout`::

    Sets the upper bound on how long a node will wait initially, or after a
    leader failure, before attempting its first election. This defaults to
    `100ms`.

`cluster.election.back_off_time`::

    Sets the amount to increase the upper bound on the wait before an election
    on each election failure. Note that this is _linear_ backoff. This defaults
    to `100ms`

`cluster.election.max_timeout`::

    Sets the maximum upper bound on how long a node will wait before attempting
    an first election, so that an network partition that lasts for a long time
    does not result in excessively sparse elections. This defaults to `10s`

`cluster.election.duration`::

    Sets how long each election is allowed to take before a node considers it
    to have failed and schedules a retry. This defaults to `500ms`.

[float]
=== Fault detection

An elected master periodically checks each of its followers in order to ensure
that they are still connected and healthy, and in turn each follower
periodically checks the health of the elected master. Elasticsearch allows for
these checks occasionally to fail or timeout without taking any action, and
will only consider a node to be truly faulty after a number of consecutive
checks have failed. The following settings control the behaviour of fault
detection.

`cluster.fault_detection.follower_check.interval`::

    Sets how long the elected master waits between checks of its followers.
    Defaults to `1s`.

`cluster.fault_detection.follower_check.timeout`::

    Sets how long the elected master waits for a response to a follower check
    before considering it to have failed. Defaults to `30s`.

`cluster.fault_detection.follower_check.retry_count`::

    Sets how many consecutive follower check failures must occur before the
    elected master considers a follower node to be faulty and removes it from
    the cluster. Defaults to `3`.

`cluster.fault_detection.leader_check.interval`::

    Sets how long each follower node waits between checks of its leader.
    Defaults to `1s`.

`cluster.fault_detection.leader_check.timeout`::

    Sets how long each follower node waits for a response to a leader check
    before considering it to have failed. Defaults to `30s`.

`cluster.fault_detection.leader_check.retry_count`::

    Sets how many consecutive leader check failures must occur before a
    follower node considers the elected master to be faulty and attempts to
    find or elect a new master. Defaults to `3`.


[float]
=== Discovery settings

TODO move this to the discovery module docs

Discovery operates in two phases: First, each node "probes" the addresses of
all known nodes by connecting to each address and attempting to identify the
node to which it is connected. Secondly it shares with the remote node a list
of all of its peers and the remote node responds with _its_ peers in turn. The
node then probes all the new nodes about which it just discovered, requests
their peers, and so on, until it has discovered an elected master node or
enough other masterless nodes that it can perform an election. If neither of
these occur quickly enough then it tries again. This process is controlled by
the following settings.

`discovery.probe.connect_timeout`::

    Sets how long to wait when attempting to connect to each address. Defaults
    to `3s`.

`discovery.probe.handshake_timeout`::

    Sets how long to wait when attempting to identify the remote node via a
    handshake. Defaults to `1s`.

`discovery.find_peers_interval`::

    Sets how long a node will wait before attempting another discovery round.

`discovery.request_peers_timeout`::

    Sets how long a node will wait after asking its peers again before
    considering the request to have failed.

[float]
=== Miscellaneous timeouts

`cluster.join.timeout`::

    Sets how long a node will wait after sending a request to join a cluster
    before it considers the request to have failed and retries. Defaults to
    `60s`.

`cluster.publish.timeout`::

    Sets how long the elected master will wait after publishing a cluster state
    update to receive acknowledgements from all its followers. If this timeout
    occurs then the elected master may start to calculate and publish a
    subsequent cluster state update, as long as it received enough
    acknowledgements to know that the previous publication was committed; if it
    did not receive enough acknowledgements to commit the update then it stands
    down as the elected leader.
