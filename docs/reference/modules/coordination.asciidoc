[[modules-cluster-coordination]]
== Cluster coordination

The cluster coordination module is responsible for electing a master node and
managing changes to the cluster state.





[float]
=== Cluster bootstrapping

When a brand-new cluster starts up for the first time, one of the tasks it must
perform is to elect its first master node, for which it needs to know the set
of master-eligible nodes whose votes should count in this first election. This
initial voting configuration is known as the _bootstrap configuration_.

It is important that the bootstrap configuration identifies exactly which nodes
should vote in the first election, and it is not sufficient to configure each
node with an expectation of how many nodes there should be in the cluster. It
is also important to note that the bootstrap configuration must come from
outside the cluster: there is no safe way for the cluster to determine the
bootstrap configuration correctly on its own.

If the bootstrap configuration is not set correctly then there is a risk when
starting up a brand-new cluster is that you accidentally form two separate
clusters instead of one. This could lead to data loss: you might start using
both clusters before noticing that anything had gone wrong, and it will then be
impossible to merge them together later.

NOTE: To illustrate the problem with configuring each node to expect a certain
cluster size, imagine starting up a three-node cluster in which each node knows
that it is going to be part of a three-node cluster. A majority of three nodes
is two, so normally the first two nodes to discover each other will form a
cluster and the third node will join them a short time later. However, imagine
that four nodes were erroneously started instead of three: in this case there
are enough nodes to form two separate clusters. Of course if each node is
started manually then it's unlikely that too many nodes are started, but it's
certainly possible to get into this situation if using a more automated
orchestrator, particularly if the orchestrator is not resilient to failures
such as network partitions.

The cluster bootstrapping process is only required the very first time a whole
cluster starts up: new nodes joining an established cluster can safely obtain
all the information they need from the elected master, and nodes that have
previously been part of a cluster will have stored to disk all the information
required when restarting.

A cluster can be bootstrapped by setting the names or addresses of the initial
set of master nodes in the `elasticsearch.yml` file:

[source]
--------------------------------------------------
cluster.initial_master_nodes:
  - master-a
  - master-b
  - master-c
--------------------------------------------------

This only needs to be set on a single master-eligible node in the cluster, but
for robustness it is safe to set this on every node in the cluster.  However
**it is vitally important** to use exactly the same set of nodes in each
configuration file.

WARNING: You must put exactly the same set of master nodes in each
configuration file in order to be sure that only a single cluster forms during
bootstrapping and therefore to avoid the risk of data loss.

It is also possible to set the initial set of master nodes on the command-line
used to start Elasticsearch:

[source]
--------------------------------------------------
$ bin/elasticsearch -Ecluster.initial_master_nodes=master-a,master-b,master-c
--------------------------------------------------


If the cluster is running with a completely default configuration then it will
automatically bootstrap based on the nodes that could be discovered within a
short time after startup. Since nodes may not always reliably discover each
other quickly enough this automatic bootstrapping is not always reliable and
cannot be used in production deployments.

[float]
=== Unsafe disaster recovery

In a disaster situation a cluster may have lost half or more of its
master-eligible nodes and therefore be in a state in which it cannot elect a
master. There is no way to recover from this situation without risking data
loss (including the loss of indexed documents) but if there is no other viable
path forwards then this may be necessary. This can be performed with the
following command on a surviving node:

[source,js]
--------------------------------------------------
POST /_cluster/force_local_node_takeover
--------------------------------------------------
// CONSOLE

This forcibly overrides the current voting configuration with one in which the
handling node is the only voting master, so that it forms a quorum on its own.
Because there is a risk of data loss when performing this command it requires
the `accept_data_loss` parameter to be set to `true` in the URL.


