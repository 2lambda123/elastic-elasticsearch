---
"line_29":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: POST
        path: "_analyze"
        body: |
          {
            "analyzer": "pattern",
            "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "the",
              "start_offset": 0,
              "end_offset": 3,
              "type": "word",
              "position": 0
            },
            {
              "token": "2",
              "start_offset": 4,
              "end_offset": 5,
              "type": "word",
              "position": 1
            },
            {
              "token": "quick",
              "start_offset": 6,
              "end_offset": 11,
              "type": "word",
              "position": 2
            },
            {
              "token": "brown",
              "start_offset": 12,
              "end_offset": 17,
              "type": "word",
              "position": 3
            },
            {
              "token": "foxes",
              "start_offset": 18,
              "end_offset": 23,
              "type": "word",
              "position": 4
            },
            {
              "token": "jumped",
              "start_offset": 24,
              "end_offset": 30,
              "type": "word",
              "position": 5
            },
            {
              "token": "over",
              "start_offset": 31,
              "end_offset": 35,
              "type": "word",
              "position": 6
            },
            {
              "token": "the",
              "start_offset": 36,
              "end_offset": 39,
              "type": "word",
              "position": 7
            },
            {
              "token": "lazy",
              "start_offset": 40,
              "end_offset": 44,
              "type": "word",
              "position": 8
            },
            {
              "token": "dog",
              "start_offset": 45,
              "end_offset": 48,
              "type": "word",
              "position": 9
            },
            {
              "token": "s",
              "start_offset": 49,
              "end_offset": 50,
              "type": "word",
              "position": 10
            },
            {
              "token": "bone",
              "start_offset": 51,
              "end_offset": 55,
              "type": "word",
              "position": 11
            }
          ]
        }
---
"line_180":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "my-index-000001"
        body: |
          {
            "settings": {
              "analysis": {
                "analyzer": {
                  "my_email_analyzer": {
                    "type":      "pattern",
                    "pattern":   "\\W|_",
                    "lowercase": true
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: POST
        path: "my-index-000001/_analyze"
        body: |
          {
            "analyzer": "my_email_analyzer",
            "text": "John_Smith@foo-bar.com"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "john",
              "start_offset": 0,
              "end_offset": 4,
              "type": "word",
              "position": 0
            },
            {
              "token": "smith",
              "start_offset": 5,
              "end_offset": 10,
              "type": "word",
              "position": 1
            },
            {
              "token": "foo",
              "start_offset": 11,
              "end_offset": 14,
              "type": "word",
              "position": 2
            },
            {
              "token": "bar",
              "start_offset": 15,
              "end_offset": 18,
              "type": "word",
              "position": 3
            },
            {
              "token": "com",
              "start_offset": 19,
              "end_offset": 22,
              "type": "word",
              "position": 4
            }
          ]
        }
---
"line_267":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "my-index-000001"
        body: |
          {
            "settings": {
              "analysis": {
                "analyzer": {
                  "camel": {
                    "type": "pattern",
                    "pattern": "([^\\p{L}\\d]+)|(?<=\\D)(?=\\d)|(?<=\\d)(?=\\D)|(?<=[\\p{L}&&[^\\p{Lu}]])(?=\\p{Lu})|(?<=\\p{Lu})(?=\\p{Lu}[\\p{L}&&[^\\p{Lu}]])"
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: GET
        path: "my-index-000001/_analyze"
        body: |
          {
            "analyzer": "camel",
            "text": "MooseX::FTPClass2_beta"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "moose",
              "start_offset": 0,
              "end_offset": 5,
              "type": "word",
              "position": 0
            },
            {
              "token": "x",
              "start_offset": 5,
              "end_offset": 6,
              "type": "word",
              "position": 1
            },
            {
              "token": "ftp",
              "start_offset": 8,
              "end_offset": 11,
              "type": "word",
              "position": 2
            },
            {
              "token": "class",
              "start_offset": 11,
              "end_offset": 16,
              "type": "word",
              "position": 3
            },
            {
              "token": "2",
              "start_offset": 16,
              "end_offset": 17,
              "type": "word",
              "position": 4
            },
            {
              "token": "beta",
              "start_offset": 18,
              "end_offset": 22,
              "type": "word",
              "position": 5
            }
          ]
        }
---
"line_385":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "pattern_example"
        body: |
          {
            "settings": {
              "analysis": {
                "tokenizer": {
                  "split_on_non_word": {
                    "type":       "pattern",
                    "pattern":    "\\W+"
                  }
                },
                "analyzer": {
                  "rebuilt_pattern": {
                    "tokenizer": "split_on_non_word",
                    "filter": [
                      "lowercase"
                    ]
                  }
                }
              }
            }
          }
  - is_false: _shards.failures

  - compare_analyzers: {index: pattern_example, first: pattern, second: rebuilt_pattern}

