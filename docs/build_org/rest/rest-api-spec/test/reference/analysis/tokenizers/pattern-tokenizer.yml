---
"line_32":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: POST
        path: "_analyze"
        body: |
          {
            "tokenizer": "pattern",
            "text": "The foo_bar_size's default is 5."
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "The",
              "start_offset": 0,
              "end_offset": 3,
              "type": "word",
              "position": 0
            },
            {
              "token": "foo_bar_size",
              "start_offset": 4,
              "end_offset": 16,
              "type": "word",
              "position": 1
            },
            {
              "token": "s",
              "start_offset": 17,
              "end_offset": 18,
              "type": "word",
              "position": 2
            },
            {
              "token": "default",
              "start_offset": 19,
              "end_offset": 26,
              "type": "word",
              "position": 3
            },
            {
              "token": "is",
              "start_offset": 27,
              "end_offset": 29,
              "type": "word",
              "position": 4
            },
            {
              "token": "5",
              "start_offset": 30,
              "end_offset": 31,
              "type": "word",
              "position": 5
            }
          ]
        }
---
"line_128":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "my-index-000001"
        body: |
          {
            "settings": {
              "analysis": {
                "analyzer": {
                  "my_analyzer": {
                    "tokenizer": "my_tokenizer"
                  }
                },
                "tokenizer": {
                  "my_tokenizer": {
                    "type": "pattern",
                    "pattern": ","
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: POST
        path: "my-index-000001/_analyze"
        body: |
          {
            "analyzer": "my_analyzer",
            "text": "comma,separated,values"
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "comma",
              "start_offset": 0,
              "end_offset": 5,
              "type": "word",
              "position": 0
            },
            {
              "token": "separated",
              "start_offset": 6,
              "end_offset": 15,
              "type": "word",
              "position": 1
            },
            {
              "token": "values",
              "start_offset": 16,
              "end_offset": 22,
              "type": "word",
              "position": 2
            }
          ]
        }
---
"line_216":
  - skip:
      features:
        - default_shards
        - stash_in_key
        - stash_in_path
        - stash_path_replace
        - warnings
  - do:
      raw:
        method: PUT
        path: "my-index-000001"
        body: |
          {
            "settings": {
              "analysis": {
                "analyzer": {
                  "my_analyzer": {
                    "tokenizer": "my_tokenizer"
                  }
                },
                "tokenizer": {
                  "my_tokenizer": {
                    "type": "pattern",
                    "pattern": "\"((?:\\\\\"|[^\"]|\\\\\")+)\"",
                    "group": 1
                  }
                }
              }
            }
          }
  - is_false: _shards.failures
  - do:
      raw:
        method: POST
        path: "my-index-000001/_analyze"
        body: |
          {
            "analyzer": "my_analyzer",
            "text": "\"value\", \"value with embedded \\\" quote\""
          }
  - is_false: _shards.failures
  - match:
      $body:
        {
          "tokens": [
            {
              "token": "value",
              "start_offset": 1,
              "end_offset": 6,
              "type": "word",
              "position": 0
            },
            {
              "token": "value with embedded \\\" quote",
              "start_offset": 10,
              "end_offset": 38,
              "type": "word",
              "position": 1
            }
          ]
        }
